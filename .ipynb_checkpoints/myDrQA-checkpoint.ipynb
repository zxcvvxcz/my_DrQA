{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "import os\n",
    "import tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [00:20<00:00, 4231.57lines/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.experimental.datasets import SQuAD1\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "# data_dir = '.data'\n",
    "# data_names = ['dev-v1.1.json', 'train-v1.1.json']\n",
    "# for data_name in data_names:\n",
    "#     if not os.path.isfile(os.path.join(data_dir, data_name)):\n",
    "#         print('download')\n",
    "#         train, dev = SQuAD1()\n",
    "#         break\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "# dataset shape: (paragraph, question, answer, span)\n",
    "train, dev = SQuAD1(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab = train.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# errors = 0\n",
    "# print('length of vocab before filtering:', len(vocab.stoi))\n",
    "# for key, value in list(vocab.stoi.items()):\n",
    "#     if re.search('\\n', key) or re.search(' ', key):\n",
    "#         errors += 1\n",
    "#         print(key)\n",
    "#         vocab.stoi.pop(key)\n",
    "#         vocab.itos.pop(value)\n",
    "#         vocab.freqs.pop(key)\n",
    "#         # vocab.freqs[key] -= 1\n",
    "#         # if vocab.freqs[key] < 1:\n",
    "#         #     vocab.freqs.pop(key)\n",
    "#\n",
    "# print(errors)\n",
    "# print('length of vocab after filtering:', len(vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train, dev = SQuAD1(vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = [(len(paragraph), len(question), idx, paragraph, question, answer, span)\n",
    "            for idx, (paragraph, question, answer, span) in enumerate(train)]\n",
    "# train_data.sort() # sort by length and pad sequences with similar lengths\n",
    "# paragpraph, question: tensor of indices of words, use itos to get word\n",
    "# Generate the pad id\n",
    "pad_id = vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(train_data[0][3])\n",
    "# for idx in train_data[0][3]:\n",
    "#     print(train.get_vocab().itos[idx], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pad_data(data):\n",
    "    # Find max length of the mini-batch\n",
    "    # train.get_vocab()['pad'], dev.get_vocab()['pad'] is equal to 22949\n",
    "    max_p_len = max(list(zip(*data))[0])\n",
    "    max_q_len = max(list(zip(*data))[1])\n",
    "    paragraph_list = list(zip(*data))[3]\n",
    "    question_list = list(zip(*data))[4]\n",
    "    answer_list = list(zip(*data))[5]\n",
    "    span_list = list(zip(*data))[6]\n",
    "    padded_paragraphs = torch.stack([torch.cat((paragraph,\n",
    "            torch.LongTensor([pad_id] * (max_p_len - len(paragraph))))) \\\n",
    "            for paragraph in paragraph_list])\n",
    "    padded_questions = torch.stack([torch.cat((question,\n",
    "            torch.tensor([pad_id] * (max_q_len - len(question))).long())) \\\n",
    "            for question in question_list])\n",
    "    paragraph_pad_mask = torch.zeros_like(padded_paragraphs).masked_fill(padded_paragraphs == pad_id, 1)\n",
    "    question_pad_mask = torch.zeros_like(padded_questions).masked_fill(padded_questions == pad_id, 1)\n",
    "\n",
    "    return padded_paragraphs, padded_questions, span_list, answer_list, \\\n",
    "           paragraph_pad_mask, question_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "from torch.utils.data import DataLoader\n",
    "trainloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_data)\n",
    "testloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for i, (p, q, a, s) in enumerate(train):\n",
    "#     print(p,q,a,s)\n",
    "#     nps = s[0].numpy()\n",
    "#     tokens = tokenizer(train.data[i][0])\n",
    "#     print(tokens[int(nps[0])])\n",
    "#     print(tokens[nps[1]])\n",
    "#     if i > 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([tensor([32, 33])], [tensor([27, 27])], [tensor([217, 221])], [tensor([19, 20])], [tensor([24, 24])], [tensor([81, 81])], [tensor([87, 89])], [tensor([47, 63])], [tensor([45, 45])], [tensor([16, 16])], [tensor([57, 57])], [tensor([41, 44])], [tensor([13, 14])], [tensor([50, 50])], [tensor([3, 8])], [tensor([0, 1])], [tensor([79, 80])], [tensor([118, 130])], [tensor([13, 13])], [tensor([36, 36])], [tensor([18, 20])], [tensor([158, 161])], [tensor([0, 0])], [tensor([74, 76])], [tensor([31, 31])], [tensor([44, 45])], [tensor([ 9, 16])], [tensor([68, 68])], [tensor([111, 111])], [tensor([124, 126])], [tensor([27, 28])], [tensor([102, 102])])\n",
      "([tensor([21, 22])], [tensor([105, 107])], [tensor([110, 111])], [tensor([110, 110])], [tensor([57, 58])], [tensor([77, 78])], [tensor([31, 33])], [tensor([ 3, 11])], [tensor([41, 42])], [tensor([36, 36])], [tensor([54, 57])], [tensor([162, 162])], [tensor([66, 67])], [tensor([64, 83])], [tensor([37, 37])], [tensor([29, 45])], [tensor([25, 26])], [tensor([9, 9])], [tensor([216, 217])], [tensor([9, 9])], [tensor([35, 38])], [tensor([108, 110])], [tensor([0, 2])], [tensor([80, 80])], [tensor([75, 76])], [tensor([33, 33])], [tensor([20, 22])], [tensor([95, 95])], [tensor([0, 3])], [tensor([29, 29])], [tensor([7, 9])], [tensor([167, 168])])\n",
      "([tensor([4, 7])], [tensor([55, 56])], [tensor([21, 23])], [tensor([52, 54])], [tensor([61, 61])], [tensor([0, 1])], [tensor([ 6, 18])], [tensor([39, 43])], [tensor([37, 39])], [tensor([ 95, 100])], [tensor([29, 29])], [tensor([49, 51])], [tensor([66, 83])], [tensor([12, 13])], [tensor([10, 12])], [tensor([22, 22])], [tensor([ 98, 100])], [tensor([45, 49])], [tensor([128, 132])], [tensor([211, 212])], [tensor([40, 41])], [tensor([344, 346])], [tensor([73, 73])], [tensor([31, 42])], [tensor([1, 2])], [tensor([154, 154])], [tensor([68, 68])], [tensor([35, 35])], [tensor([67, 69])], [tensor([52, 53])], [tensor([75, 76])], [tensor([28, 28])])\n"
     ]
    }
   ],
   "source": [
    "for idx, (padded_paragraphs, padded_questions, span_list, answer_list,\n",
    "           paragraph_pad_mask, question_pad_mask) in enumerate(trainloader):\n",
    "    # print(idx, padded_paragraphs, padded_questions, span_list, answer_list,\n",
    "    #        paragraph_pad_mask, question_pad_mask)\n",
    "    print(span_list)\n",
    "    if idx > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(train.get_vocab()['pad'], dev.get_vocab()['pad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "glove_vec = torchtext.vocab.GloVe(name='840B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_word_embedding(vocab, pre_trained_emb_vec):\n",
    "    print(pre_trained_emb_vec.dim)\n",
    "    weights_matrix = np.zeros((len(vocab), pre_trained_emb_vec.dim))\n",
    "    words_found = 0\n",
    "    no_word = 0\n",
    "    for i, (word, _) in enumerate(vocab.freqs.most_common()):\n",
    "        try:\n",
    "            word_index = pre_trained_emb_vec.stoi[word]\n",
    "            weights_matrix[i] = pre_trained_emb_vec[word_index]\n",
    "            words_found += 1\n",
    "        except:\n",
    "            no_word += 1 # no such word in pre_trained_embedding: zero vector\n",
    "    print('words not found:', no_word)\n",
    "    print('words found:', words_found)\n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for key, value in vocab.freqs.items():\n",
    "#     if re.search(' ', key):\n",
    "#         print(key, value)\n",
    "# for i, word in enumerate(vocab.freqs.most_common()):\n",
    "#     print(word)\n",
    "#     if i > 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "words not found: 17435\n",
      "words found: 86591\n"
     ]
    }
   ],
   "source": [
    "word_emb_table = build_word_embedding(vocab, glove_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# glove_vec.vectors[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def exact_match(paragraphs_indices, questions_indices, itos):\n",
    "    # process one paragraph sentence, one question sentence\n",
    "    exact_match_table = np.zeros((len(paragraphs_indices), len(questions_indices[0])))\n",
    "    for i, (paragraph_indices, question_indices) in \\\n",
    "            enumerate(zip(paragraphs_indices, questions_indices)):\n",
    "        print(paragraphs_indices)\n",
    "        print(paragraphs_indices.size())\n",
    "        paragraph_sentence = ' '.join(word for word in itos[paragraph_indices])\n",
    "        question_sentence = ' '.join(word for word in itos[question_indices])\n",
    "        paragraph_sentence_uncased = paragraph_sentence.lower()\n",
    "        question_sentence_uncased = question_sentence.lower()\n",
    "        paragraph_processed = nlp(paragraph_sentence)\n",
    "        question_processed = nlp(question_sentence)\n",
    "        question_lemmas = [lem.lemma_ for lem in question_processed]\n",
    "        paragraph_uncased_processed = nlp(paragraph_sentence_uncased)\n",
    "        question_uncased_processed = nlp(question_sentence_uncased)\n",
    "        for j, paragraph_word, paragraph_word_uncased in \\\n",
    "                enumerate(zip(paragraph_processed, paragraph_uncased_processed)):\n",
    "            if paragraph_word.text == '<pad>':\n",
    "                continue\n",
    "            em_tensor = torch.LongTensor([0, 0, 0])\n",
    "            # original\n",
    "            if paragraph_word.text in question_sentence:\n",
    "                em_tensor[0] = 1\n",
    "            # lemma\n",
    "            if paragraph_word.lemma_ in question_lemmas:\n",
    "                em_tensor[1] = 1\n",
    "            # uncased\n",
    "            if paragraph_word_uncased.text in question_sentence_uncased:\n",
    "                em_tensor[2] = 1\n",
    "            exact_match_table[i][j] = em_tensor\n",
    "\n",
    "    return exact_match_table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AlignedQuestionEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, paragraph, question, question_pad_mask):\n",
    "        p = self.relu(self.linear(paragraph))\n",
    "\n",
    "        q = self.relu(self.linear(question))\n",
    "        q = question.permute(0, 2, 1)\n",
    "\n",
    "        dot_product = torch.bmm(p, q)\n",
    "        dot_product = dot_product.masked_fill(question_pad_mask == 1, -np.inf)\n",
    "\n",
    "        dot_product_flatten = dot_product.view(-1, question.size(1))\n",
    "\n",
    "        attn_score = F.softmax(dot_product_flatten, dim=1)\n",
    "        attn_score = attn_score.view(-1, paragraph.shape[1], question.shape[1])\n",
    "\n",
    "        aligned_embedding = torch.bmm(attn_score, question)\n",
    "        return aligned_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "P_ENCODING_NUM = 2\n",
    "class MultiLayerBiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nlayers, dropout):\n",
    "        super().__init__()\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "        self.lstms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstms.append(nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True))\n",
    "        for i in range(1, nlayers):\n",
    "            self.lstms.append(nn.LSTM(hidden_size * 2, hidden_size,\n",
    "                                      batch_first=True, bidirectional=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_output = self.dropout(x)\n",
    "        lstm_output, (hidden_state, cell_state) = self.lstms[0](lstm_output)\n",
    "        hidden_states = [hidden_state]\n",
    "\n",
    "        for i in range(1, self.nlayers):\n",
    "            lstm_output = self.dropout(hidden_state)\n",
    "            lstm_output, (hidden_state, cell_state) = self.lstms[i](lstm_output)\n",
    "            hidden_states.append(hidden_state)\n",
    "\n",
    "        output = torch.cat(hidden_states, dim=2)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QuestionEncoding(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, nlayers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "        self.lstm = MultiLayerBiLSTM(input_size, hidden_size, nlayers, dropout)\n",
    "\n",
    "    def forward(self, x, question_mask):\n",
    "        x_lstm = self.lstm(x)\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.linear(x) # attention score\n",
    "        x = x.masked_fill(question_mask == 1, -np.inf) # masking\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "        encoding = torch.bmm(x_lstm, x)\n",
    "        encoding = encoding.squeeze(1)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PredictionLayer(nn.Module):\n",
    "    def __init__(self, p_size, q_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(q_size, p_size)\n",
    "\n",
    "    def forward(self, paragraph, question, paragraph_mask):\n",
    "        Wq = self.linear(question)\n",
    "        Wq = Wq.unsqueeze(2)\n",
    "        pWq = paragraph.bmm(Wq)\n",
    "        pWq = pWq.squeeze(2)\n",
    "        pWq = pWq.masked_fill(paragraph_mask == 1, -np.inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fix_embedding(grad):\n",
    "    grad[1000:] = 0\n",
    "    return grad\n",
    "\n",
    "class DocumentReader(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, nlayers, paragraph_encoding_num, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.p_word_embedding_layer = nn.Embedding.from_pretrained(torch.FloatTensor(word_emb_table).to(device), freeze=False)\n",
    "        self.p_word_embedding_layer.register_backward_hook(fix_embedding)\n",
    "        self.aligned_embedding_layer = AlignedQuestionEmbedding(embedding_size)\n",
    "        self.paragraph_lstm = MultiLayerBiLSTM(embedding_size * 2 + 3, hidden_size, nlayers, dropout)\n",
    "        # self.paragraph_lstm = MultiLayerBiLSTM(embedding_size * 2, hidden_size, nlayers, dropout)\n",
    "\n",
    "        self.question_encoder = QuestionEncoding(embedding_size, hidden_size, nlayers, dropout)\n",
    "\n",
    "        self.prediction_layer_start = PredictionLayer(hidden_size * nlayers * paragraph_encoding_num,\n",
    "                                                          hidden_size * nlayers * paragraph_encoding_num)\n",
    "        self.prediction_layer_end = PredictionLayer(hidden_size * nlayers * paragraph_encoding_num,\n",
    "                                                        hidden_size * nlayers * paragraph_encoding_num)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, paragraph, question, paragraph_mask, question_mask):\n",
    "        em_embedding = exact_match(paragraph, question, vocab.itos)\n",
    "        p_word_embedding = self.p_word_embedding_layer(paragraph)\n",
    "        aligned_embedding = self.aligned_embedding_layer(paragraph, question, paragraph_mask)\n",
    "\n",
    "        paragraph_embeddings = torch.cat([em_embedding, p_word_embedding, aligned_embedding])\n",
    "        paragraph_encoding = self.paragraph_lstm(paragraph_embeddings)\n",
    "\n",
    "        question_encoding = self.question_encoder(question, question_mask)\n",
    "\n",
    "        prediction_start = self.prediction_layer_start(paragraph_encoding, question_encoding, paragraph_mask)\n",
    "        prediction_end = self.prediction_layer_end(paragraph_encoding, question_encoding, paragraph_mask)\n",
    "\n",
    "        return prediction_start, prediction_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "EMB_SIZE = 300\n",
    "NLAYERS = 3\n",
    "PARAGRAPH_EMBEDDING_NUM = 2\n",
    "DROPOUT = 0.3\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DocumentReader(HIDDEN_SIZE,\n",
    "                       EMB_SIZE,\n",
    "                       NLAYERS,\n",
    "                       PARAGRAPH_EMBEDDING_NUM,\n",
    "                       DROPOUT,\n",
    "                       device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataset):\n",
    "    '''\n",
    "    Trains the model.\n",
    "    '''\n",
    "\n",
    "    print(\"Start training ........\")\n",
    "\n",
    "    train_loss = 0.\n",
    "\n",
    "    # put the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # iterate through training data\n",
    "    for i, (paragraphs, questions, span_list, answer_list,\n",
    "            paragraph_mask, question_mask) in enumerate(train_dataset):\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Starting batch: {i}\")\n",
    "\n",
    "\n",
    "        # place the tensors on GPU\n",
    "        paragraphs = paragraphs.to(device)\n",
    "        paragraph_mask = paragraph_mask.to(device)\n",
    "        questions = questions.to(device)\n",
    "        question_mask = question_mask.to(device)\n",
    "        # span_list = span_list.to(device)\n",
    "\n",
    "        # forward pass, get the predictions\n",
    "        preds = model(paragraphs, questions, paragraph_mask, question_mask)\n",
    "\n",
    "        start_pred, end_pred = preds\n",
    "\n",
    "        # separate labels for start and end position\n",
    "        start_span, end_span = span_list[:,0], span_list[:,1]\n",
    "\n",
    "        # calculate loss\n",
    "        loss = F.cross_entropy(start_pred, start_span) + F.cross_entropy(end_pred, end_span)\n",
    "\n",
    "        # backward pass, calculates the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "\n",
    "        # update the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # zero the gradients to prevent them from accumulating\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss/len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ........\n",
      "Starting batch: 0\n",
      "tensor([[  243,  1319,     9,  ...,     1,     1,     1],\n",
      "        [    2, 14318,   557,  ...,     1,     1,     1],\n",
      "        [    7,     0,  1819,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    2,   363,   101,  ...,     1,     1,     1],\n",
      "        [    0,    72,  3418,  ...,     1,     1,     1],\n",
      "        [    2,   164,   265,  ...,     1,     1,     1]], device='cuda:0')\n",
      "torch.Size([32, 301])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-544b5cdb359b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-b2da7dbb4b22>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataset)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# forward pass, get the predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparagraph_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mstart_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pch33\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-4b7d73e1eb1b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, paragraph, question, paragraph_mask, question_mask)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparagraph_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mem_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexact_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mp_word_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_word_embedding_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0maligned_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maligned_embedding_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparagraph_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-5d243e630527>\u001b[0m in \u001b[0;36mexact_match\u001b[1;34m(paragraphs_indices, questions_indices, itos)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraphs_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraphs_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mparagraph_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparagraph_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mquestion_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquestion_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mparagraph_sentence_uncased\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparagraph_sentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "train_loss = train(model, trainloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# trainloader length: 2700\n",
    "# testloader length: 323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "import os\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [00:21<00:00, 4052.90lines/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.experimental.datasets import SQuAD1\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "# data_dir = '.data'\n",
    "# data_names = ['dev-v1.1.json', 'train-v1.1.json']\n",
    "# for data_name in data_names:\n",
    "#     if not os.path.isfile(os.path.join(data_dir, data_name)):\n",
    "#         print('download')\n",
    "#         train, dev = SQuAD1()\n",
    "#         break\n",
    "# trainset, devset = SQuAD1()\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "# dataset shape: (paragraph, question, answer, span)\n",
    "trainset, devset = SQuAD1(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab = trainset.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Tesla',\n 'could',\n 'be',\n 'harsh',\n 'at',\n 'times',\n 'and',\n 'openly',\n 'expressed',\n 'disgust',\n 'for',\n 'overweight',\n 'people',\n ',',\n 'such',\n 'as',\n 'when',\n 'he',\n 'fired',\n 'a',\n 'secretary',\n 'because',\n 'of',\n 'her',\n 'weight.:110',\n 'He',\n 'was',\n 'quick',\n 'to',\n 'criticize',\n 'clothing',\n ';',\n 'on',\n 'several',\n 'occasions',\n ',',\n 'Tesla',\n 'directed',\n 'a',\n 'subordinate',\n 'to',\n 'go',\n 'home',\n 'and',\n 'change',\n 'her',\n 'dress.:33']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Tesla could be harsh at times and openly expressed disgust for overweight people, such as when he fired a secretary because of her weight.:110 He was quick to criticize clothing; on several occasions, Tesla directed a subordinate to go home and change her dress.:33')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "whitespace_string_dev = []\n",
    "whitespace_string_train = []\n",
    "for i, (p, q, a, s) in enumerate(devset.data):\n",
    "    if(len(re.findall('  ', p + q)) > 0):\n",
    "        whitespace_string_dev.append(i)\n",
    "    for answer in a:\n",
    "        if(len(re.findall('  ', answer)) > 0):\n",
    "            whitespace_string_dev.append(i)\n",
    "            break\n",
    "for i, (p, q, a, s) in enumerate(trainset.data):\n",
    "    if(len(re.findall('  ', p + q)) > 0):\n",
    "        whitespace_string_train.append(i)\n",
    "    for answer in a:\n",
    "        if(len(re.findall('  ', answer)) > 0):\n",
    "            whitespace_string_train.append(i)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[235, 408, 593, 1094, 1300, 1304, 1305, 1306, 1307, 1308, 1309, 1690, 1766, 1807, 1815, 1868, 1869, 1870, 2170, 2184, 2187, 2245, 2266, 2323, 2380, 2383, 2390, 2433, 2486, 2516, 2582, 2623, 2766, 2865, 2877, 2878, 2879, 2880, 2881, 2955, 3019, 3031, 3076, 3077, 3078, 3079, 3336, 3496, 3503, 3509, 3517, 3521, 3550, 3575, 3595, 3672, 3675, 3754, 4138, 4360, 4518, 4524, 4533, 4630, 4631, 4637, 4645, 4659, 4665, 4834, 4865, 4866, 4880, 4882, 4883, 4884, 4885, 4886, 4987, 5386, 5387, 5388, 5389, 5390, 5565, 5987, 5988, 5989, 5990, 6576, 6794, 7350, 7550, 8050, 8455, 8456, 8457, 8457, 8458, 9026, 9027, 9027, 9028, 9029, 9030, 9051, 9052, 9053, 9054, 9055, 9080, 9460, 9645, 9646, 9647, 9648, 9649, 10084, 10171, 10272, 10275, 10380, 10391, 10471, 10472, 10473, 10474, 10475, 10480, 10481, 10482, 10483, 10493, 10504, 10539, 10540, 10541, 10542, 10546, 10547, 10548, 10549, 10550]\n",
      "143\n",
      "1322\n"
     ]
    }
   ],
   "source": [
    "print(whitespace_string_dev)\n",
    "print(len(whitespace_string_dev))\n",
    "print(len(whitespace_string_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([   14, 20129, 11824,     3,    29,   182,     2,  6206,     7,  7502,\n            16,  1562,  1578,    17,     3,    13,  9130,    22,  6354,     3,\n         20196,   522,  4493,  3893, 20734,     5,  4493,  3893, 19284, 40929,\n          5422,    43,    46,     4,    36,   513,  1620,     3, 11234,    19,\n             0,  6328,     6, 43325,    19,     0,     3,   109,  1605,     9,\n          2009,    11,   136,     6,   996,    11,   609,  2506,   347, 92741,\n            16,  2349,  3632,     3,   312, 43325,    17,     3,     9,  2009,\n            11,   426,   312,     0,     3,     6,     9,  2009,    11,   513,\n         19284,  4399,     4, 98271,     5,  5422,    23,   609, 11546,    27,\n         14499,   179, 11764, 39332,     3,    57,  6239,     9,  2009,    11,\n           136,  7047,  5446,    19,     0,  6328,     6,   860, 92741,     3,\n             6,  1012,  5686,  9918,     0,     3,  3573,     3,    57,  6239,\n          4488,  5446,    19, 55942,  6328,     6,   312, 92741,    44,     0,\n            41, 19633,    19,  1509,  6328,     6,  1022,  1666,     0,    19,\n         65828,  6328,     5,   400,  1290, 11546,   309, 12147,     0,     0,\n            16,  6460, 22973,    19, 52960,  6328,    17,     3, 20327,     0,\n             0,    16,  1669, 22973,    19, 32570,  6328,     6,   264, 92741,\n            17,     3,     6,   172,    11,    82,  5686, 23668,  2502,    16,\n          1669, 22973,    19, 66185,  6328,    17,     5,    14, 20129,     0,\n          1440,  4493,  3893,  1372,   322,  7320,  8437,     3,    57,   182,\n             2,   378,    22,     0, 43325,  6328,     6,   522, 92741,     7,\n          1048,   596,     3,   251,    22,  4493,  3893,     0,  5600, 36499,\n             3,    57, 19633,    19, 12608,  6328,     6,  6239,   759,  5446,\n            19,   275, 15601,  6328,     5,   881,    23,  4054,   356,    41,\n          1440,    63,  4493,  3893, 20734,    68,   676,  5436,     0,     6,\n          6196,     0,  5069,     5]),\n tensor([  121,    54, 22973,    49,     0,   844,  1209,    19,     2,   535,\n           236,    18]),\n [tensor([6460]), tensor([6460]), tensor([6460])],\n [tensor([124, 124]), tensor([151, 151]), tensor([151, 151])])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devset[235]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(\"The Panthers offense, which led the NFL in scoring (500 points), was loaded with talent, boasting six Pro Bowl selections. Pro Bowl quarterback Cam Newton had one of his best seasons, throwing for 3,837 yards and rushing for 636, while recording a career-high and league-leading 45 total touchdowns (35 passing, 10 rushing), a career-low 10 interceptions, and a career-best quarterback rating of 99.4. Newton's leading receivers were tight end Greg Olsen, who caught a career-high 77 passes for 1,104 yards and seven touchdowns, and wide receiver Ted Ginn, Jr., who caught 44 passes for 739 yards and 10 touchdowns; Ginn also rushed for 60 yards and returned 27 punts for 277 yards. Other key receivers included veteran Jerricho Cotchery (39 receptions for 485 yards), rookie Devin Funchess (31 receptions for 473 yards and five touchdowns), and second-year receiver Corey Brown (31 receptions for 447 yards). The Panthers backfield featured Pro Bowl running back Jonathan Stewart, who led the team with 989 rushing yards and six touchdowns in 13 games, along with Pro Bowl fullback Mike Tolbert, who rushed for 256 yards and caught 18 passes for another 154 yards. Carolina's offensive line also featured two Pro Bowl selections: center Ryan Kalil and guard Trai Turner.\",\n 'How many receptions did Cotchery  get for the 2015 season?',\n ['39', '39', '39'],\n [588, 739, 739])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devset.data[235]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# vocab.stoi[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "whitespace_tokens_dev = []\n",
    "whitespace_tokens_train = []\n",
    "blank_token = vocab.stoi[' ']\n",
    "for i, (p, q, a, _) in enumerate(devset):\n",
    "    if (blank_token in p) or (blank_token in q):\n",
    "        whitespace_tokens_dev.append(i)\n",
    "    for answer in a:\n",
    "        if blank_token in answer:\n",
    "            whitespace_tokens_dev.append(i)\n",
    "            break\n",
    "for i, (p, q, a, _) in enumerate(trainset):\n",
    "    if (blank_token in p) or (blank_token in q):\n",
    "        whitespace_tokens_train.append(i)\n",
    "    for answer in a:\n",
    "        if blank_token in answer:\n",
    "            whitespace_tokens_train.append(i)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      "1530\n"
     ]
    }
   ],
   "source": [
    "print(len(whitespace_tokens_dev))\n",
    "print(len(whitespace_tokens_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# errors = 0\n",
    "# print('length of vocab before filtering:', len(vocab.stoi))\n",
    "# for key, value in list(vocab.stoi.items()):\n",
    "#     if re.search('\\n', key) or re.search(' ', key):\n",
    "#         errors += 1\n",
    "#         print(key)\n",
    "#         vocab.stoi.pop(key)\n",
    "#         vocab.itos.pop(value)\n",
    "#         vocab.freqs.pop(key)\n",
    "#         # vocab.freqs[key] -= 1\n",
    "#         # if vocab.freqs[key] < 1:\n",
    "#         #     vocab.freqs.pop(key)\n",
    "#\n",
    "# print(errors)\n",
    "# print('length of vocab after filtering:', len(vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# trainset, devset = SQuAD1(vocab=vocab)\n",
    "# model = None\n",
    "# optimizer = None\n",
    "# loss = None\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_large_text(data):\n",
    "    return data[0] <= 400\n",
    "def remove_wrong_whitespace(data):\n",
    "    blank_token = vocab.stoi[' ']\n",
    "    p_length, q_length, idx, paragraph, question, answers, spans = data\n",
    "    if (blank_token in paragraph) or (blank_token in question):\n",
    "        return False\n",
    "    for answer in answers:\n",
    "        if blank_token in answer:\n",
    "            return False\n",
    "    return True\n",
    "def check_train_data(data):\n",
    "    # data might be wrong because of spacy tokenizer\n",
    "    p_length, q_length, idx, paragraph, question, answer, span = data\n",
    "    if span[0][0] > p_length or span[0][1] > p_length:\n",
    "        return False\n",
    "    if paragraph[span[0][0]] == answer[0][0] and paragraph[span[0][1]] == answer[0][-1]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_dev_data(data):\n",
    "    p_length, q_length, idx, paragraph, question, answers, spans = data\n",
    "    if len(spans) != 3 or len(answers) != 3:\n",
    "        return False\n",
    "    else:\n",
    "        for span, answer in zip(spans, answers):\n",
    "            if span[0] > p_length or span[1] > p_length:\n",
    "                return False\n",
    "            if paragraph[span[0]] != answer[0] or paragraph[span[1]] != answer[-1]:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85130\n",
      "8262\n"
     ]
    }
   ],
   "source": [
    "train_data = [(len(paragraph), len(question), idx, paragraph, question, answer, span)\n",
    "            for idx, (paragraph, question, answer, span) in enumerate(trainset)]\n",
    "dev_data = [(len(paragraph), len(question), idx, paragraph, question, answer, span)\n",
    "            for idx, (paragraph, question, answer, span) in enumerate(devset)]\n",
    "\n",
    "# train_data = list(filter(remove_large_text, train_data))\n",
    "# dev_data = list(filter(remove_large_text, dev_data))\n",
    "\n",
    "train_data = list(filter(remove_wrong_whitespace, train_data))\n",
    "dev_data = list(filter(remove_wrong_whitespace, dev_data))\n",
    "\n",
    "train_data = list(filter(check_train_data, train_data))\n",
    "dev_data = list(filter(check_dev_data, dev_data))\n",
    "\n",
    "\n",
    "# train_data.sort() # sort by length and pad sequences with similar lengths\n",
    "# dev_data.sort()\n",
    "# paragraph, question: tensor of indices of words, use itos to get word\n",
    "print(len(train_data))\n",
    "print(len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(train_data[0][3])\n",
    "# for idx in train_data[0][3]:\n",
    "#     print(train.get_vocab().itos[idx], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pad_data(data):\n",
    "    # Generate the pad id\n",
    "    pad_id = vocab['<pad>']\n",
    "    # Find max length of the mini-batch\n",
    "    # train.get_vocab()['pad'], dev.get_vocab()['pad'] is equal to 22949\n",
    "    max_p_len = max(list(zip(*data))[0])\n",
    "    max_q_len = max(list(zip(*data))[1])\n",
    "    paragraph_list = list(zip(*data))[3]\n",
    "    question_list = list(zip(*data))[4]\n",
    "    answer_list = list(zip(*data))[5]\n",
    "    span_list = list(zip(*data))[6]\n",
    "    padded_paragraphs = torch.stack([torch.cat((paragraph,\n",
    "            torch.LongTensor([pad_id] * (max_p_len - len(paragraph))))) \\\n",
    "            for paragraph in paragraph_list])\n",
    "    padded_questions = torch.stack([torch.cat((question,\n",
    "            torch.tensor([pad_id] * (max_q_len - len(question))).long())) \\\n",
    "            for question in question_list])\n",
    "    paragraph_pad_mask = torch.zeros_like(padded_paragraphs).masked_fill(padded_paragraphs == pad_id, 1)\n",
    "    question_pad_mask = torch.zeros_like(padded_questions).masked_fill(padded_questions == pad_id, 1)\n",
    "\n",
    "    return padded_paragraphs, padded_questions, span_list, answer_list, \\\n",
    "           paragraph_pad_mask, question_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "from torch.utils.data import DataLoader\n",
    "trainloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_data, num_workers=0)\n",
    "testloader = DataLoader(dev_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_data, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for i, (p, q, a, s) in enumerate(devset.data):\n",
    "#     print(p)\n",
    "#     print(q)\n",
    "#     print(a,s)\n",
    "#     nps = s[0].numpy()\n",
    "#     tokens = tokenizer(trainset.data[i][0])\n",
    "#     print(tokens[int(nps[0])])\n",
    "#     print(tokens[nps[1]])\n",
    "#     # print(tokens[a[0].numpy().item()])\n",
    "#     if i > 0:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for i, (p, q, a, s) in enumerate(devset):\n",
    "#     print(p)\n",
    "#     print(q)\n",
    "#     print(a,s)\n",
    "#     nps = s[0].numpy()\n",
    "#     tokens = tokenizer(trainset.data[i][0])\n",
    "#     print(tokens[int(nps[0])])\n",
    "#     print(tokens[nps[1]])\n",
    "#     # print(tokens[a[0].numpy().item()])\n",
    "#     if i > 0:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for idx, (padded_paragraphs, padded_questions, span_list, answer_list,\n",
    "#            paragraph_pad_mask, question_pad_mask) in enumerate(trainloader):\n",
    "#     # print(idx, padded_paragraphs, padded_questions, span_list, answer_list,\n",
    "#     #        paragraph_pad_mask, question_pad_mask)\n",
    "#     # print(padded_paragraphs.masked_fill(paragraph_pad_mask == 1, -1))\n",
    "#     if idx > 0:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(trainset.get_vocab()['pad'], dev.get_vocab()['pad'])\n",
    "# devset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2196016/2196017 [03:51<00:00, 9478.21it/s] \n"
     ]
    }
   ],
   "source": [
    "glove_vec = torchtext.vocab.GloVe(name='840B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_word_embedding(vocab, pre_trained_emb_vec):\n",
    "    # print(pre_trained_emb_vec.dim)\n",
    "    weights_matrix = np.zeros((len(vocab), pre_trained_emb_vec.dim))\n",
    "    words_found = 0\n",
    "    no_word = 0\n",
    "    for i, word in enumerate(vocab.itos):\n",
    "        try:\n",
    "            weights_matrix[i] = pre_trained_emb_vec[word]\n",
    "            words_found += 1\n",
    "        except:\n",
    "            no_word += 1 # no such word in pre_trained_embedding: zero vector\n",
    "    print('words not found:', no_word)\n",
    "    print('words found:', words_found)\n",
    "    # weights_matrix\n",
    "    return torch.FloatTensor(weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "most_common = vocab.freqs.most_common()\n",
    "for w, c in most_common:\n",
    "    if w == '<unk>' or w == '<pad>':\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for key, value in vocab.freqs.items():\n",
    "#     if re.search(' ', key):\n",
    "#         print(key, value)\n",
    "# for i, word in enumerate(vocab.freqs.most_common()):\n",
    "#     print(word)\n",
    "#     if i > 5:\n",
    "#         break\n",
    "# the, the_i = vocab.freqs.most_common()[5]\n",
    "# word_index = glove_vec.stoi[the]\n",
    "# print(word_index)\n",
    "# print(glove_vec[the])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words not found: 0\n",
      "words found: 104028\n"
     ]
    }
   ],
   "source": [
    "word_emb_table = build_word_embedding(vocab, glove_vec)\n",
    "np.save('mydrqaglove_vt.npy',word_emb_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99350\n"
     ]
    }
   ],
   "source": [
    "# glove_vec.vectors[:5]\n",
    "weight_matrix = np.load('drqaglove_vt.npy')\n",
    "word_emb_table = np.load('mydrqaglove_vt.npy')\n",
    "wrong_rows = []\n",
    "wrong_vectors = []\n",
    "for i, (wv1, wv2) in enumerate(zip(weight_matrix, word_emb_table)):\n",
    "    if (wv1 != wv2).any():\n",
    "        wrong_rows.append([i])\n",
    "        wrong_vectors.append([wv1, wv2])\n",
    "\n",
    "print(len(wrong_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_emb_table[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# not using now\n",
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['parser','ner',])\n",
    "#\n",
    "# def exact_match(paragraphs_indices, questions_indices, vocab):\n",
    "#     # process one paragraph batch, one question batch\n",
    "#     # print(paragraphs_indices.size())\n",
    "#     # print(questions_indices.size())\n",
    "#     #\n",
    "#     # j = 0\n",
    "#     # for (paragraph_indices, question_indices) in \\\n",
    "#     #         zip(paragraphs_indices, questions_indices):\n",
    "#     #     j += 1\n",
    "#     # print('j:',j)\n",
    "#     exact_match_table = np.zeros((len(paragraphs_indices), len(paragraphs_indices[0]), 3))\n",
    "#     # print(exact_match_table.shape)\n",
    "#\n",
    "#     for i, (paragraph_indices, question_indices) in \\\n",
    "#             enumerate(zip(paragraphs_indices, questions_indices)):\n",
    "#         # print(paragraphs_indices)\n",
    "#         # print(paragraphs_indices.size())\n",
    "#         # paragraph_processed = nlp(paragraph_sentence)\n",
    "#         # question_lemmas = [lem.lemma_ for lem in question_processed]\n",
    "#         for j, paragraph_index in enumerate(paragraph_indices):\n",
    "#             paragraph_word = vocab.itos[paragraph_index]\n",
    "#             if paragraph_word == '<pad>':\n",
    "#                 # print('got pad')\n",
    "#                 continue\n",
    "#             em_tensor = torch.LongTensor([0, 0, 0])\n",
    "#             # original\n",
    "#             if paragraph_index in question_indices:\n",
    "#                 em_tensor[0] = 1\n",
    "#             # lemma\n",
    "#             if vocab.stoi[nlp(paragraph_word)[0].lemma_] in question_indices:\n",
    "#                 em_tensor[1] = 1\n",
    "#             # uncased\n",
    "#             if vocab.stoi[paragraph_word.lower()] and \\\n",
    "#                     vocab.stoi[paragraph_word.lower()] in question_indices:\n",
    "#                 em_tensor[2] = 1\n",
    "#             exact_match_table[i][j] = em_tensor\n",
    "#\n",
    "#     return torch.LongTensor(exact_match_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AlignedQuestionEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(input_dim, input_dim)\n",
    "        self.linear2 = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, paragraph, question, question_pad_mask):\n",
    "\n",
    "        p = self.relu(self.linear1(paragraph))\n",
    "\n",
    "        # q = self.relu(self.linear2(question))\n",
    "\n",
    "        q = self.relu(self.linear1(question))\n",
    "        q = q.permute(0, 2, 1)\n",
    "\n",
    "        dot_product = torch.bmm(p, q)\n",
    "        # print(dot_product.size())\n",
    "        # print(question_pad_mask.size())\n",
    "        question_mask_expand = question_pad_mask.unsqueeze(1).expand(dot_product.size())\n",
    "        dot_product = dot_product.masked_fill(question_mask_expand == 1, -float('inf'))\n",
    "\n",
    "        attn_score = F.softmax(dot_product.view(-1, question.size(1)), dim=1)\n",
    "        attn_score = attn_score.view(-1, paragraph.shape[1], question.shape[1])\n",
    "\n",
    "        aligned_embedding = torch.bmm(attn_score, question)\n",
    "        return aligned_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiLayerBiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nlayers, dropout):\n",
    "        super().__init__()\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "        self.lstms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstms.append(nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True))\n",
    "        for i in range(1, nlayers):\n",
    "            self.lstms.append(nn.LSTM(hidden_size * 2, hidden_size,\n",
    "                                      batch_first=True, bidirectional=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        lstm_output, _ = self.lstms[0](x)\n",
    "        hidden_states = [lstm_output]\n",
    "        # print(lstm_output.size(), hidden_state.size(), cell_state.size())\n",
    "        for i in range(1, self.nlayers):\n",
    "            # lstm_output = self.dropout(lstm_output)\n",
    "            lstm_output, _ = self.lstms[i](lstm_output)\n",
    "            # print(lstm_output.size(), hidden_state.size(), cell_state.size())\n",
    "            hidden_states.append(lstm_output)\n",
    "\n",
    "        output = torch.cat(hidden_states, dim=2)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QuestionEncoding(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, nlayers, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm_output_size = hidden_size * 6\n",
    "        self.linear = nn.Linear(self.lstm_output_size, 1)\n",
    "        self.lstm = MultiLayerBiLSTM(input_size, hidden_size, nlayers, dropout)\n",
    "        # biLSTM output size: hidden size * 6\n",
    "    def forward(self, x, question_mask):\n",
    "        try:\n",
    "            x = self.lstm(x)\n",
    "            b = x.contiguous().view(-1, x.size(-1))\n",
    "            b = self.linear(b) # attention score\n",
    "            b = b.view(x.shape[0], -1)\n",
    "            # print(x.size(), question_mask.size())\n",
    "            b = b.masked_fill(question_mask == 1, -float('inf')) # masking\n",
    "            b = F.softmax(b, dim=1)\n",
    "\n",
    "            encoding = b.unsqueeze(1).bmm(x).squeeze(1)\n",
    "            # print(x.size(), x_lstm.size())\n",
    "            return encoding\n",
    "        except:\n",
    "            print('question mask size:', question_mask.size())\n",
    "            print('x size:', x.size())\n",
    "            print('b size:', b.size())\n",
    "            print(traceback.print_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PredictionLayer(nn.Module):\n",
    "    def __init__(self, p_size, q_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(q_size, p_size)\n",
    "\n",
    "    def forward(self, paragraph, question, paragraph_mask):\n",
    "        Wq = self.linear(question)\n",
    "        pWq = paragraph.bmm(Wq.unsqueeze(2)).squeeze(2)\n",
    "        pWq = pWq.masked_fill(paragraph_mask == 1, -float('inf'))\n",
    "        return pWq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def fixate_embedding(grad):\n",
    "    grad[1000:] = 0\n",
    "    return grad\n",
    "\n",
    "class DocumentReader(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, nlayers, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.word_embedding_layer = nn.Embedding.from_pretrained(torch.FloatTensor(word_emb_table).to(device), freeze=False)\n",
    "        self.word_embedding_layer.weight.register_hook(fixate_embedding)\n",
    "        # print(embedding_size)\n",
    "        self.aligned_embedding_layer = AlignedQuestionEmbedding(embedding_size)\n",
    "        # self.paragraph_lstm = MultiLayerBiLSTM(embedding_size * 2 + 3, hidden_size, nlayers, dropout)\n",
    "        self.paragraph_lstm = MultiLayerBiLSTM(embedding_size * 2, hidden_size, nlayers, dropout)\n",
    "\n",
    "        self.question_encoder = QuestionEncoding(embedding_size, hidden_size, nlayers, dropout)\n",
    "\n",
    "        self.prediction_layer_start = PredictionLayer(hidden_size * nlayers * 2,\n",
    "                                                          hidden_size * nlayers * 2)\n",
    "        self.prediction_layer_end = PredictionLayer(hidden_size * nlayers * 2,\n",
    "                                                        hidden_size * nlayers * 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, paragraph, question, paragraph_mask, question_mask):\n",
    "        # em_embedding = exact_match(paragraph, question, vocab)\n",
    "        # print(em_embedding.size())\n",
    "        p_word_embedding = self.word_embedding_layer(paragraph)\n",
    "        q_word_embedding = self.word_embedding_layer(question)\n",
    "        p_word_embedding = self.dropout(p_word_embedding)\n",
    "        q_word_embedding = self.dropout(q_word_embedding)\n",
    "        aligned_embedding = self.aligned_embedding_layer(p_word_embedding, q_word_embedding, question_mask)\n",
    "        # print(p_word_embedding.size())\n",
    "        # print(aligned_embedding.size())\n",
    "        paragraph_embeddings = torch.cat([p_word_embedding, aligned_embedding], dim=2)\n",
    "\n",
    "        # paragraph_embeddings = torch.cat([em_embedding.to(device), p_word_embedding.to(device), aligned_embedding.to(device)], dim=2)\n",
    "        paragraph_encoding = self.paragraph_lstm(paragraph_embeddings)\n",
    "        # print(question.size(), question_mask.size())\n",
    "        question_encoding = self.question_encoder(q_word_embedding, question_mask)\n",
    "\n",
    "        prediction_start = self.prediction_layer_start(paragraph_encoding, question_encoding, paragraph_mask)\n",
    "        prediction_end = self.prediction_layer_end(paragraph_encoding, question_encoding, paragraph_mask)\n",
    "\n",
    "        return prediction_start, prediction_end"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "EMB_SIZE = 300\n",
    "NLAYERS = 3\n",
    "DROPOUT = 0.3\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DocumentReader(HIDDEN_SIZE,\n",
    "                       EMB_SIZE,\n",
    "                       NLAYERS,\n",
    "                       DROPOUT,\n",
    "                       device).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# dataiter = iter(trainloader)\n",
    "# dataiter_next = dataiter.next()\n",
    "# print(dataiter_next)\n",
    "# (p, q, a, s, p_mask, q_mask) = dataiter.next()\n",
    "# writer.add_graph(model, p, p_mask, q_mask)\n",
    "# writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax(model.parameters())\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min', factor=0.1, patience=2, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "def train(model, train_dataset):\n",
    "    '''\n",
    "    Trains the model.\n",
    "    '''\n",
    "\n",
    "    print(\"Start training ........\")\n",
    "\n",
    "    train_loss = 0.\n",
    "\n",
    "    # put the model in training mode\n",
    "    model.train()\n",
    "    start_time = time()\n",
    "    # iterate through training data\n",
    "    try:\n",
    "        for i, (paragraphs, questions, span_list, answer_list,\n",
    "                paragraph_mask, question_mask) in enumerate(train_dataset):\n",
    "            # if i < 575:\n",
    "            #     continue\n",
    "            if i % 500 == 0:\n",
    "                print(f\"Starting batch: {i}, time: {time() - start_time}\")\n",
    "\n",
    "\n",
    "            # place the tensors on GPU\n",
    "            paragraphs = paragraphs.to(device)\n",
    "            paragraph_mask = paragraph_mask.to(device)\n",
    "            questions = questions.to(device)\n",
    "            question_mask = question_mask.to(device)\n",
    "            # span_list = span_list.to(device)\n",
    "\n",
    "            # forward pass, get the predictions\n",
    "            preds = model(paragraphs, questions, paragraph_mask, question_mask)\n",
    "\n",
    "            start_pred, end_pred = preds\n",
    "\n",
    "            # print('preds:', start_pred, end_pred)\n",
    "            # separate labels for start and end position\n",
    "            span_start = []\n",
    "            span_end = []\n",
    "            for span in span_list:\n",
    "                span_start.append(span[0][0].item())\n",
    "                span_end.append(span[0][1].item())\n",
    "\n",
    "            # print('span:', span_start, span_end)\n",
    "            span_start = torch.LongTensor(span_start).to(device)\n",
    "            span_end = torch.LongTensor(span_end).to(device)\n",
    "            # calculate loss\n",
    "            loss = F.cross_entropy(start_pred, span_start) + F.cross_entropy(end_pred, span_end)\n",
    "\n",
    "            # backward pass, calculates the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "\n",
    "            # update the gradients\n",
    "            optimizer.step()\n",
    "            # zero the gradients to prevent them from accumulating\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "    except Exception as e:\n",
    "        print(f'sizes of pred:{start_pred.size()} / span:{span_start.size()}')\n",
    "        print(f'span_start: {span_start[23]}\\nspan_end: {span_end[23]}')\n",
    "        print(f'i: {i}')\n",
    "        print(f'paragraph: {paragraphs}')\n",
    "        # bad_p = paragraphs.numpy()[23]\n",
    "        # bad_q = questions.numpy()[23]\n",
    "        # bad_p_text = [vocab.itos[pi] for pi in bad_p]\n",
    "        # bad_q_text = [vocab.itos[qi] for qi in bad_q]\n",
    "        # bad_p_text = ' '.join(bad_p_text)\n",
    "        # bad_q_text = ' '.join(bad_q_text)\n",
    "        #\n",
    "        # print(bad_p_text)\n",
    "        # print(bad_q_text)\n",
    "        # print(f'paragraph size: {paragraphs.size()}, question size: {questions.size()}')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "\n",
    "    return train_loss / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %time train_loss = train(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validate(model, test_dataset):\n",
    "    '''\n",
    "    Validates the model.\n",
    "    '''\n",
    "\n",
    "    print(\"Start validation ........\")\n",
    "\n",
    "    val_loss = 0.\n",
    "    emScore = 0\n",
    "    f1Score = 0\n",
    "    # put the model in eval mode\n",
    "    model.eval()\n",
    "    start_time = time()\n",
    "    # iterate through training data\n",
    "    for i, (paragraphs, questions, span_list, answer_list,\n",
    "            paragraph_mask, question_mask) in enumerate(test_dataset):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Starting batch: {i}, time: {time() - start_time}\")\n",
    "\n",
    "\n",
    "        # place the tensors on GPU\n",
    "        paragraphs = paragraphs.to(device)\n",
    "        paragraph_mask = paragraph_mask.to(device)\n",
    "        questions = questions.to(device)\n",
    "        question_mask = question_mask.to(device)\n",
    "        # span_list = span_list.to(device)\n",
    "\n",
    "        # forward pass, get the predictions\n",
    "        preds = model(paragraphs, questions, paragraph_mask, question_mask)\n",
    "\n",
    "        start_pred, end_pred = preds\n",
    "        # print('preds:', start_pred, end_pred)\n",
    "        log_softmax = nn.LogSoftmax(dim=1) # batchwise log softmax\n",
    "        pred_table = log_softmax(start_pred).unsqueeze(2) + log_softmax(end_pred).unsqueeze(1)\n",
    "        pred_mask1 = (torch.ones_like(pred_table) * -float('inf')).tril(diagonal=-1)# start index <= end index\n",
    "        pred_mask2 = (torch.ones_like(pred_table) * -float('inf')).triu(diagonal=16)\n",
    "        pred_table += pred_mask1 + pred_mask2\n",
    "\n",
    "        start_pred_argmax = []\n",
    "        end_pred_argmax = []\n",
    "        paragraph_length = pred_table.shape[-1]\n",
    "        for batch in pred_table:\n",
    "            arg_max = batch.argmax()\n",
    "            start_pred_argmax.append(arg_max // paragraph_length)\n",
    "            end_pred_argmax.append(arg_max % paragraph_length)\n",
    "\n",
    "        # separate labels for start and end position\n",
    "        span_start = []\n",
    "        span_end = []\n",
    "        true_answers_list = []\n",
    "        my_answers = []\n",
    "        for paragraph, spans, answers, sp, ep in \\\n",
    "                zip(paragraphs, span_list, answer_list, start_pred_argmax, end_pred_argmax):\n",
    "            span_start.append([span[0].item() for span in spans][:3])\n",
    "            span_end.append([span[1].item() for span in spans][:3])\n",
    "            true_answers_list.append([ans2txt(answer) for answer in answers])\n",
    "            if sp > ep or ep > sp + 15:\n",
    "                print(f'wrong range, sp:{sp}, ep:{ep} ')\n",
    "            my_answers.append(span2txt([sp, ep + 1], paragraph))\n",
    "        with torch.no_grad():\n",
    "            # print('span:', span_start, span_end)\n",
    "            try:\n",
    "                span_start = torch.LongTensor(span_start).to(device)\n",
    "                span_end = torch.LongTensor(span_end).to(device)\n",
    "                # calculate loss\n",
    "                loss = [F.cross_entropy(start_pred, span_start.t()[i]) +\n",
    "                        F.cross_entropy(end_pred, span_end.t()[i]) for i in range(3)]\n",
    "                loss = min(loss)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                emScore += em_batch(my_answers, true_answers_list)\n",
    "                f1Score += f1_batch(my_answers, true_answers_list)\n",
    "            except:\n",
    "                print('start pred:', start_pred)\n",
    "                print('start pred shape:', start_pred.shape)\n",
    "                print('span_list:', span_list)\n",
    "                print('span_list length:', len(span_list))\n",
    "                print('span_start:', span_start)\n",
    "                print('span_start shape:', np.asarray(span_start).shape)\n",
    "                print('span_end:', span_end)\n",
    "                print(traceback.format_exc())\n",
    "\n",
    "    return val_loss / len(test_dataset), emScore / len(test_dataset), f1Score / len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def normalize_answer(s):\n",
    "    s = s.lower()\n",
    "    s = s.translate(str.maketrans('','',punctuation))\n",
    "    s = re.sub(r'\\b(a|an|the)\\b', ' ', s)\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "def em_batch(my_answers, true_answers_list):\n",
    "    # true_answers_list: batch size * 3\n",
    "    em = 0\n",
    "    for my_answer, true_answers in zip(my_answers, true_answers_list):\n",
    "        for true_answer in true_answers:\n",
    "            if my_answer == true_answer:\n",
    "                em += 1\n",
    "                break\n",
    "    return em / BATCH_SIZE\n",
    "\n",
    "def f1_batch(my_answers, true_answers_list):\n",
    "    f1Batch = 0\n",
    "    for my_answer, true_answers in zip(my_answers, true_answers_list):\n",
    "        f1_single = 0\n",
    "        for true_answer in true_answers:\n",
    "            my_answer_split = my_answer.split()\n",
    "            true_answer_split = true_answer.split()\n",
    "            common = Counter(my_answer_split) & Counter(true_answer_split)\n",
    "            num_intersection = sum(common.values())\n",
    "            if num_intersection == 0:\n",
    "                continue\n",
    "            precision = num_intersection / len(my_answer_split)\n",
    "            recall = num_intersection / len(true_answer_split)\n",
    "            f1_single = max((2 * precision * recall) / (precision + recall), f1_single)\n",
    "        f1Batch += f1_single\n",
    "        # if f1_single < 0.9:\n",
    "        #     print('my answer split:', my_answer_split)\n",
    "        #     print('true answer split:', true_answer_split)\n",
    "    return f1Batch / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def span2txt(span, paragraph):\n",
    "    # print(span[0].item())\n",
    "    my_answer = paragraph[int(span[0].item()) : int(span[1].item()) + 1]\n",
    "    return ans2txt(my_answer)\n",
    "def ans2txt(answer):\n",
    "    words = []\n",
    "    for a_index in answer:\n",
    "        words.append(vocab.itos[a_index.item()])\n",
    "    return normalize_answer(' '.join(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "best_val_loss = 100\n",
    "path = 'best.pt'\n",
    "if os.path.isfile(path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "else:\n",
    "    epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring epoch 0\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.004999399185180664\n",
      "Starting batch: 500, time: 78.88674902915955\n",
      "Starting batch: 1000, time: 162.00155425071716\n",
      "Starting batch: 1500, time: 248.92445707321167\n",
      "Starting batch: 2000, time: 333.687922000885\n",
      "Starting batch: 2500, time: 418.6049211025238\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.0009999275207519531\n",
      "train_loss: 4.224119443635392, val_loss: 3.119059269032423\n",
      "em_score: 34.19401544401545, f1_score: 63.65416949311778\n",
      "End epoch 0, elapsed time: 456.4718987941742\n",
      "Staring epoch 1\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.008944034576416016\n",
      "Starting batch: 500, time: 82.10528635978699\n",
      "Starting batch: 1000, time: 163.64307689666748\n",
      "Starting batch: 1500, time: 245.59665727615356\n",
      "Starting batch: 2000, time: 328.1354978084564\n",
      "Starting batch: 2500, time: 414.41203689575195\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.001976490020751953\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\caffe2\\serialize\\inline_container.cc:274] . unexpected pos 142976192 vs 142976080",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32mc:\\users\\pch33\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36msave\u001B[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001B[0m\n\u001B[0;32m    371\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0m_open_zipfile_writer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopened_file\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mopened_zipfile\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 372\u001B[1;33m                 \u001B[0m_save\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mopened_zipfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpickle_module\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpickle_protocol\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    373\u001B[0m                 \u001B[1;32mreturn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\pch33\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36m_save\u001B[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001B[0m\n\u001B[0;32m    490\u001B[0m         \u001B[0mnum_bytes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstorage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mstorage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0melement_size\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 491\u001B[1;33m         \u001B[0mzip_file\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite_record\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstorage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata_ptr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_bytes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    492\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-76-f14a8b3720e7>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mbest_val_loss\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mval_loss\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m         torch.save({\n\u001B[0m\u001B[0;32m     20\u001B[0m             \u001B[1;34m'epoch'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m             \u001B[1;34m'model_state_dict'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\pch33\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36msave\u001B[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001B[0m\n\u001B[0;32m    371\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0m_open_zipfile_writer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopened_file\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mopened_zipfile\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    372\u001B[0m                 \u001B[0m_save\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mopened_zipfile\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpickle_module\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpickle_protocol\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 373\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    374\u001B[0m         \u001B[0m_legacy_save\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mopened_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpickle_module\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpickle_protocol\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    375\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\pch33\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36m__exit__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    257\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    258\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__exit__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 259\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfile_like\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite_end_of_file\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    260\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuffer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflush\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    261\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: [enforce fail at ..\\caffe2\\serialize\\inline_container.cc:274] . unexpected pos 142976192 vs 142976080"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "em_scores = []\n",
    "f1_scores = []\n",
    "epoch_start = max(epoch, 0)\n",
    "for epoch in range(epoch_start, epoch_start + 50):\n",
    "    print(f'Staring epoch {epoch}')\n",
    "    start_time = time()\n",
    "\n",
    "    train_loss = train(model, trainloader)\n",
    "    val_loss, emScore, f1Score = validate(model, testloader)\n",
    "    scheduler.step(val_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    em_scores.append(emScore * 100)\n",
    "    f1_scores.append(f1Score * 100)\n",
    "\n",
    "    if best_val_loss > val_loss:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "            }, path)\n",
    "    end_time = time()\n",
    "\n",
    "    time_elapsed = end_time - start_time\n",
    "    print(f'train_loss: {train_loss}, val_loss: {val_loss}')\n",
    "    print(f'em_score: {emScore * 100}, f1_score: {f1Score * 100}')\n",
    "    print(f'End epoch {epoch}, elapsed time: {time_elapsed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('result.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(train_losses)\n",
    "    writer.writerow(val_losses)\n",
    "    writer.writerow(em_scores)\n",
    "    writer.writerow(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# val_loss, emScore, f1Score = validate(model, testloader)\n",
    "# print(f'train_loss: {train_loss}, val_loss: {val_loss}')\n",
    "# print(f'em_score: {emScore * 100}, f1_score: {f1Score * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(epoch - len(train_losses) + 1, epoch + 1)], train_losses)\n",
    "plt.plot([i for i in range(epoch - len(val_losses) + 1, epoch + 1)], val_losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('CE losses')\n",
    "plt.title('Training Result')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(epoch - len(em_scores) + 1, epoch + 1)], em_scores)\n",
    "plt.plot([i for i in range(epoch - len(f1_scores) + 1, epoch + 1)], f1_scores)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Scores on SQuAD 1.1')\n",
    "plt.legend(['EM', 'F1'])\n",
    "plt.xticks([i for i in range(1, len(f1_scores) + 1)])\n",
    "plt.yticks(np.arange(15, 70, 5))\n",
    "plt.savefig(f\"best{epoch - len(f1_scores) + 1}-{epoch}.png\", dpi=350)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# idx = 0\n",
    "# for i, t_data in enumerate(testloader):\n",
    "#     idx = i\n",
    "# print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (p, q, s, a, p_mask, q_mask), p 길이 점점 증가\n",
    "# p, q: tensor, batch size * length\n",
    "# s: tuple of tensors, batch size\n",
    "# a:\n",
    "# dataiter = iter(trainloader)\n",
    "# dataiter_next = dataiter.next()\n",
    "# dataiter_next[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataiter = iter(trainloader)\n",
    "# for i in range(100):\n",
    "#     dataiter_100 = dataiter.next()\n",
    "# dataiter_100[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# start_pred.shape\n",
    "# start_pred_argmax = torch.argmax(start_pred, dim=1)\n",
    "# print(start_pred_argmax)\n",
    "# print(span_list)\n",
    "# print(span_list[0][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for p, s, a in zip(paragraphs, span_list, answer_list):\n",
    "    i += 1\n",
    "    if i < 3:\n",
    "        print(p)\n",
    "        print(s)\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# em_score = 0\n",
    "# for i, (p, s, a) in enumerate(list(zip(paragraphs, span_list, answer_list))):\n",
    "#     i += 1\n",
    "#     # print(i)\n",
    "#     if i < 3:\n",
    "#         # print(p)\n",
    "#         # print(s)\n",
    "#         # print(a)\n",
    "#         em_score += em_func(p, a[0], s)\n",
    "# print(em_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# trainloader length: 2700\n",
    "# testloader length: 323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "import os\n",
    "import traceback\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [00:23<00:00, 3766.70lines/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.experimental.datasets import SQuAD1\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "# data_dir = '.data'\n",
    "# data_names = ['dev-v1.1.json', 'train-v1.1.json']\n",
    "# for data_name in data_names:\n",
    "#     if not os.path.isfile(os.path.join(data_dir, data_name)):\n",
    "#         print('download')\n",
    "#         train, dev = SQuAD1()\n",
    "#         break\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "# dataset shape: (paragraph, question, answer, span)\n",
    "trainset, devset = SQuAD1(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab = trainset.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# errors = 0\n",
    "# print('length of vocab before filtering:', len(vocab.stoi))\n",
    "# for key, value in list(vocab.stoi.items()):\n",
    "#     if re.search('\\n', key) or re.search(' ', key):\n",
    "#         errors += 1\n",
    "#         print(key)\n",
    "#         vocab.stoi.pop(key)\n",
    "#         vocab.itos.pop(value)\n",
    "#         vocab.freqs.pop(key)\n",
    "#         # vocab.freqs[key] -= 1\n",
    "#         # if vocab.freqs[key] < 1:\n",
    "#         #     vocab.freqs.pop(key)\n",
    "#\n",
    "# print(errors)\n",
    "# print('length of vocab after filtering:', len(vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# trainset, devset = SQuAD1(vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_large_text(data):\n",
    "    return data[0] <= 400\n",
    "\n",
    "def check_train_data(data):\n",
    "    # data might be wrong because of spacy tokenizer\n",
    "    p_length, q_length, idx, paragraph, question, answer, span = data\n",
    "    if span[0][0] > p_length or span[0][1] > p_length:\n",
    "        return False\n",
    "    if paragraph[span[0][0]] == answer[0][0] and paragraph[span[0][1]] == answer[0][-1]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_dev_data(data):\n",
    "    p_length, q_length, idx, paragraph, question, answers, spans = data\n",
    "    if len(spans) != 3 or len(answers) != 3:\n",
    "        return False\n",
    "    else:\n",
    "        for span, answer in zip(spans, answers):\n",
    "            if span[0] > p_length or span[1] > p_length:\n",
    "                return False\n",
    "            if paragraph[span[0]] != answer[0] or paragraph[span[1]] != answer[-1]:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86412\n",
      "8295\n"
     ]
    }
   ],
   "source": [
    "train_data = [(len(paragraph), len(question), idx, paragraph, question, answer, span)\n",
    "            for idx, (paragraph, question, answer, span) in enumerate(trainset)]\n",
    "dev_data = [(len(paragraph), len(question), idx, paragraph, question, answer, span)\n",
    "            for idx, (paragraph, question, answer, span) in enumerate(devset)]\n",
    "\n",
    "train_data = list(filter(remove_large_text, train_data))\n",
    "dev_data = list(filter(remove_large_text, dev_data))\n",
    "\n",
    "train_data = list(filter(check_train_data, train_data))\n",
    "dev_data = list(filter(check_dev_data, dev_data))\n",
    "\n",
    "\n",
    "train_data.sort() # sort by length and pad sequences with similar lengths\n",
    "dev_data.sort()\n",
    "# paragraph, question: tensor of indices of words, use itos to get word\n",
    "print(len(train_data))\n",
    "print(len(dev_data))\n",
    "# Generate the pad id\n",
    "pad_id = vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(train_data[0][3])\n",
    "# for idx in train_data[0][3]:\n",
    "#     print(train.get_vocab().itos[idx], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pad_data(data):\n",
    "    # Find max length of the mini-batch\n",
    "    # train.get_vocab()['pad'], dev.get_vocab()['pad'] is equal to 22949\n",
    "    max_p_len = max(list(zip(*data))[0])\n",
    "    max_q_len = max(list(zip(*data))[1])\n",
    "    paragraph_list = list(zip(*data))[3]\n",
    "    question_list = list(zip(*data))[4]\n",
    "    answer_list = list(zip(*data))[5]\n",
    "    span_list = list(zip(*data))[6]\n",
    "    padded_paragraphs = torch.stack([torch.cat((paragraph,\n",
    "            torch.LongTensor([pad_id] * (max_p_len - len(paragraph))))) \\\n",
    "            for paragraph in paragraph_list])\n",
    "    padded_questions = torch.stack([torch.cat((question,\n",
    "            torch.tensor([pad_id] * (max_q_len - len(question))).long())) \\\n",
    "            for question in question_list])\n",
    "    paragraph_pad_mask = torch.zeros_like(padded_paragraphs).masked_fill(padded_paragraphs == pad_id, 1)\n",
    "    question_pad_mask = torch.zeros_like(padded_questions).masked_fill(padded_questions == pad_id, 1)\n",
    "\n",
    "    return padded_paragraphs, padded_questions, span_list, answer_list, \\\n",
    "           paragraph_pad_mask, question_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "from torch.utils.data import DataLoader\n",
    "trainloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_data)\n",
    "testloader = DataLoader(dev_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([12163, 56789]), tensor([12163, 56789]), tensor([12163, 56789])] [tensor([33, 34]), tensor([33, 34]), tensor([33, 34])]\n",
      "facing\n",
      "it\n",
      "[tensor([  881, 20129]), tensor([  881, 20129]), tensor([  881, 20129])] [tensor([44, 45]), tensor([44, 45]), tensor([44, 45])]\n",
      "upraised\n",
      "with\n"
     ]
    }
   ],
   "source": [
    "for i, (p, q, a, s) in enumerate(devset):\n",
    "    print(a,s)\n",
    "    nps = s[0].numpy()\n",
    "    tokens = tokenizer(trainset.data[i][0])\n",
    "    print(tokens[int(nps[0])])\n",
    "    print(tokens[nps[1]])\n",
    "    # print(tokens[a[0].numpy().item()])\n",
    "    if i > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for idx, (padded_paragraphs, padded_questions, span_list, answer_list,\n",
    "           paragraph_pad_mask, question_pad_mask) in enumerate(trainloader):\n",
    "    # print(idx, padded_paragraphs, padded_questions, span_list, answer_list,\n",
    "    #        paragraph_pad_mask, question_pad_mask)\n",
    "    # print(padded_paragraphs.masked_fill(paragraph_pad_mask == 1, -1))\n",
    "    if idx > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(trainset.get_vocab()['pad'], dev.get_vocab()['pad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "glove_vec = torchtext.vocab.GloVe(name='840B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_word_embedding(vocab, pre_trained_emb_vec):\n",
    "    # print(pre_trained_emb_vec.dim)\n",
    "    weights_matrix = np.zeros((len(vocab), pre_trained_emb_vec.dim))\n",
    "    words_found = 0\n",
    "    no_word = 0\n",
    "    for i, (word, _) in enumerate(vocab.freqs.most_common()):\n",
    "        try:\n",
    "            word_index = pre_trained_emb_vec.stoi[word]\n",
    "            weights_matrix[i] = pre_trained_emb_vec[word_index]\n",
    "            words_found += 1\n",
    "        except:\n",
    "            no_word += 1 # no such word in pre_trained_embedding: zero vector\n",
    "    print('words not found:', no_word)\n",
    "    print('words found:', words_found)\n",
    "    return torch.FloatTensor(weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for key, value in vocab.freqs.items():\n",
    "#     if re.search(' ', key):\n",
    "#         print(key, value)\n",
    "# for i, word in enumerate(vocab.freqs.most_common()):\n",
    "#     print(word)\n",
    "#     if i > 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words not found: 17435\n",
      "words found: 86591\n"
     ]
    }
   ],
   "source": [
    "word_emb_table = build_word_embedding(vocab, glove_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# glove_vec.vectors[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# not using now\n",
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['parser','ner',])\n",
    "#\n",
    "# def exact_match(paragraphs_indices, questions_indices, vocab):\n",
    "#     # process one paragraph batch, one question batch\n",
    "#     # print(paragraphs_indices.size())\n",
    "#     # print(questions_indices.size())\n",
    "#     #\n",
    "#     # j = 0\n",
    "#     # for (paragraph_indices, question_indices) in \\\n",
    "#     #         zip(paragraphs_indices, questions_indices):\n",
    "#     #     j += 1\n",
    "#     # print('j:',j)\n",
    "#     exact_match_table = np.zeros((len(paragraphs_indices), len(paragraphs_indices[0]), 3))\n",
    "#     # print(exact_match_table.shape)\n",
    "#\n",
    "#     for i, (paragraph_indices, question_indices) in \\\n",
    "#             enumerate(zip(paragraphs_indices, questions_indices)):\n",
    "#         # print(paragraphs_indices)\n",
    "#         # print(paragraphs_indices.size())\n",
    "#         # paragraph_processed = nlp(paragraph_sentence)\n",
    "#         # question_lemmas = [lem.lemma_ for lem in question_processed]\n",
    "#         for j, paragraph_index in enumerate(paragraph_indices):\n",
    "#             paragraph_word = vocab.itos[paragraph_index]\n",
    "#             if paragraph_word == '<pad>':\n",
    "#                 # print('got pad')\n",
    "#                 continue\n",
    "#             em_tensor = torch.LongTensor([0, 0, 0])\n",
    "#             # original\n",
    "#             if paragraph_index in question_indices:\n",
    "#                 em_tensor[0] = 1\n",
    "#             # lemma\n",
    "#             if vocab.stoi[nlp(paragraph_word)[0].lemma_] in question_indices:\n",
    "#                 em_tensor[1] = 1\n",
    "#             # uncased\n",
    "#             if vocab.stoi[paragraph_word.lower()] and \\\n",
    "#                     vocab.stoi[paragraph_word.lower()] in question_indices:\n",
    "#                 em_tensor[2] = 1\n",
    "#             exact_match_table[i][j] = em_tensor\n",
    "#\n",
    "#     return torch.LongTensor(exact_match_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AlignedQuestionEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, paragraph, question, question_pad_mask):\n",
    "\n",
    "        p = self.relu(self.linear(paragraph))\n",
    "\n",
    "        q = self.relu(self.linear(question))\n",
    "        q = q.permute(0, 2, 1)\n",
    "\n",
    "        dot_product = torch.bmm(p, q)\n",
    "        # print(dot_product.size())\n",
    "        # print(question_pad_mask.size())\n",
    "        question_mask_expand = question_pad_mask.unsqueeze(1).expand(dot_product.size())\n",
    "        dot_product = dot_product.masked_fill(question_mask_expand == 1, -float('inf'))\n",
    "\n",
    "        dot_product_flatten = dot_product.view(-1, question.size(1))\n",
    "\n",
    "        attn_score = F.softmax(dot_product_flatten, dim=1)\n",
    "        attn_score = attn_score.view(-1, paragraph.shape[1], question.shape[1])\n",
    "\n",
    "        aligned_embedding = torch.bmm(attn_score, question)\n",
    "        return aligned_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiLayerBiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nlayers, dropout):\n",
    "        super().__init__()\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "        self.lstms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstms.append(nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True))\n",
    "        for i in range(1, nlayers):\n",
    "            self.lstms.append(nn.LSTM(hidden_size * 2, hidden_size,\n",
    "                                      batch_first=True, bidirectional=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.dropout(x)\n",
    "        lstm_output, (hidden_state, cell_state) = self.lstms[0](x)\n",
    "        hidden_states = [lstm_output]\n",
    "        # print(lstm_output.size(), hidden_state.size(), cell_state.size())\n",
    "        for i in range(1, self.nlayers):\n",
    "            lstm_output = self.dropout(lstm_output)\n",
    "            lstm_output, (hidden_state, cell_state) = self.lstms[i](lstm_output)\n",
    "            # print(lstm_output.size(), hidden_state.size(), cell_state.size())\n",
    "            hidden_states.append(lstm_output)\n",
    "\n",
    "        output = torch.cat(hidden_states, dim=2)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QuestionEncoding(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, nlayers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "        self.lstm = MultiLayerBiLSTM(input_size, hidden_size, nlayers, dropout)\n",
    "\n",
    "    def forward(self, x, question_mask):\n",
    "        x_lstm = self.lstm(x)\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.linear(x) # attention score\n",
    "        x = x.view(question_mask.shape[0], -1)\n",
    "        # print(x.size(), question_mask.size())\n",
    "        x = x.masked_fill(question_mask == 1, -float('inf')) # masking\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "        # print(x.size(), x_lstm.size())\n",
    "        encoding = torch.bmm(x, x_lstm)\n",
    "        encoding = encoding.squeeze(1)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PredictionLayer(nn.Module):\n",
    "    def __init__(self, p_size, q_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(q_size, p_size)\n",
    "\n",
    "    def forward(self, paragraph, question, paragraph_mask):\n",
    "        Wq = self.linear(question)\n",
    "        Wq = Wq.unsqueeze(2)\n",
    "        pWq = paragraph.bmm(Wq)\n",
    "        pWq = pWq.squeeze(2)\n",
    "        pWq = pWq.masked_fill(paragraph_mask == 1, -float('inf'))\n",
    "        return pWq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fixate_embedding(grad):\n",
    "    grad[1000:] = 0\n",
    "    return grad\n",
    "\n",
    "class DocumentReader(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, nlayers, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.word_embedding_layer = nn.Embedding.from_pretrained(torch.FloatTensor(word_emb_table).to(device), freeze=False)\n",
    "        self.word_embedding_layer.weight.register_hook(fixate_embedding)\n",
    "        # print(embedding_size)\n",
    "        self.aligned_embedding_layer = AlignedQuestionEmbedding(embedding_size)\n",
    "        # self.paragraph_lstm = MultiLayerBiLSTM(embedding_size * 2 + 3, hidden_size, nlayers, dropout)\n",
    "        self.paragraph_lstm = MultiLayerBiLSTM(embedding_size * 2, hidden_size, nlayers, dropout)\n",
    "\n",
    "        self.question_encoder = QuestionEncoding(embedding_size, hidden_size, nlayers, dropout)\n",
    "\n",
    "        self.prediction_layer_start = PredictionLayer(hidden_size * nlayers * 2,\n",
    "                                                          hidden_size * nlayers * 2)\n",
    "        self.prediction_layer_end = PredictionLayer(hidden_size * nlayers * 2,\n",
    "                                                        hidden_size * nlayers * 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, paragraph, question, paragraph_mask, question_mask):\n",
    "        # em_embedding = exact_match(paragraph, question, vocab)\n",
    "        # print(em_embedding.size())\n",
    "        p_word_embedding = self.word_embedding_layer(paragraph)\n",
    "        q_word_embedding = self.word_embedding_layer(question)\n",
    "        p_word_embedding = self.dropout(p_word_embedding)\n",
    "        q_word_embedding = self.dropout(q_word_embedding)\n",
    "        aligned_embedding = self.aligned_embedding_layer(p_word_embedding, q_word_embedding, question_mask)\n",
    "        # print(p_word_embedding.size())\n",
    "        # print(aligned_embedding.size())\n",
    "        paragraph_embeddings = torch.cat([p_word_embedding.to(device), aligned_embedding.to(device)], dim=2)\n",
    "\n",
    "        # paragraph_embeddings = torch.cat([em_embedding.to(device), p_word_embedding.to(device), aligned_embedding.to(device)], dim=2)\n",
    "        paragraph_encoding = self.paragraph_lstm(paragraph_embeddings)\n",
    "        # print(question.size(), question_mask.size())\n",
    "        question_encoding = self.question_encoder(q_word_embedding, question_mask)\n",
    "\n",
    "        prediction_start = self.prediction_layer_start(paragraph_encoding, question_encoding, paragraph_mask)\n",
    "        prediction_end = self.prediction_layer_end(paragraph_encoding, question_encoding, paragraph_mask)\n",
    "\n",
    "        return prediction_start, prediction_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "EMB_SIZE = 300\n",
    "NLAYERS = 3\n",
    "DROPOUT = 0.3\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DocumentReader(HIDDEN_SIZE,\n",
    "                       EMB_SIZE,\n",
    "                       NLAYERS,\n",
    "                       DROPOUT,\n",
    "                       device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "#\n",
    "# writer = SummaryWriter('runs/myDrQA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataiter = iter(trainloader)\n",
    "# dataiter_next = dataiter.next()\n",
    "# print(dataiter_next)\n",
    "# (p, q, a, s, p_mask, q_mask) = dataiter.next()\n",
    "# writer.add_graph(model, p, p_mask, q_mask)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "def train(model, train_dataset):\n",
    "    '''\n",
    "    Trains the model.\n",
    "    '''\n",
    "\n",
    "    print(\"Start training ........\")\n",
    "\n",
    "    train_loss = 0.\n",
    "\n",
    "    # put the model in training mode\n",
    "    model.train()\n",
    "    start_time = time()\n",
    "    # iterate through training data\n",
    "    try:\n",
    "        for i, (paragraphs, questions, span_list, answer_list,\n",
    "                paragraph_mask, question_mask) in enumerate(train_dataset):\n",
    "            # if i < 575:\n",
    "            #     continue\n",
    "            if i % 500 == 0:\n",
    "                print(f\"Starting batch: {i}, time: {time() - start_time}\")\n",
    "\n",
    "\n",
    "            # place the tensors on GPU\n",
    "            paragraphs = paragraphs.to(device)\n",
    "            paragraph_mask = paragraph_mask.to(device)\n",
    "            questions = questions.to(device)\n",
    "            question_mask = question_mask.to(device)\n",
    "            # span_list = span_list.to(device)\n",
    "\n",
    "            # forward pass, get the predictions\n",
    "            preds = model(paragraphs, questions, paragraph_mask, question_mask)\n",
    "\n",
    "            start_pred, end_pred = preds\n",
    "\n",
    "            # print('preds:', start_pred, end_pred)\n",
    "            # separate labels for start and end position\n",
    "            span_start = []\n",
    "            span_end = []\n",
    "            for span in span_list:\n",
    "                span_start.append(span[0][0].item())\n",
    "                span_end.append(span[0][1].item())\n",
    "\n",
    "            # print('span:', span_start, span_end)\n",
    "            span_start = torch.LongTensor(span_start).to(device)\n",
    "            span_end = torch.LongTensor(span_end).to(device)\n",
    "            # calculate loss\n",
    "            loss = F.cross_entropy(start_pred, span_start) + F.cross_entropy(end_pred, span_end)\n",
    "\n",
    "            # backward pass, calculates the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "\n",
    "            # update the gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the gradients to prevent them from accumulating\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # ...학습 중 손실(running loss)을 기록하고\n",
    "            # writer.add_scalar('training loss',\n",
    "            #                 train_loss / 500,\n",
    "            #                 epoch * len(trainloader) + i)\n",
    "            train_loss += loss.item()\n",
    "    except Exception as e:\n",
    "        print(f'sizes of pred:{start_pred.size()} / span:{span_start.size()}')\n",
    "        print(f'span_start: {span_start[23]}\\nspan_end: {span_end[23]}')\n",
    "        print(f'i: {i}')\n",
    "        print(f'paragraph: {paragraphs}')\n",
    "        bad_p = paragraphs.numpy()[23]\n",
    "        bad_q = questions.numpy()[23]\n",
    "        bad_p_text = [vocab.itos[pi] for pi in bad_p]\n",
    "        bad_q_text = [vocab.itos[qi] for qi in bad_q]\n",
    "        bad_p_text = ' '.join(bad_p_text)\n",
    "        bad_q_text = ' '.join(bad_q_text)\n",
    "\n",
    "        print(bad_p_text)\n",
    "        print(bad_q_text)\n",
    "        print(f'paragraph size: {paragraphs.size()}, question size: {questions.size()}')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "\n",
    "    return train_loss / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %time train_loss = train(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def validate(model, test_dataset):\n",
    "    '''\n",
    "    Validates the model.\n",
    "    '''\n",
    "\n",
    "    print(\"Start validation ........\")\n",
    "\n",
    "    val_loss = 0.\n",
    "    emScore = 0\n",
    "    f1Score = 0\n",
    "    # put the model in eval mode\n",
    "    model.eval()\n",
    "    start_time = time()\n",
    "    # iterate through training data\n",
    "    for i, (paragraphs, questions, span_list, answer_list,\n",
    "            paragraph_mask, question_mask) in enumerate(test_dataset):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Starting batch: {i}, time: {time() - start_time}\")\n",
    "\n",
    "\n",
    "        # place the tensors on GPU\n",
    "        paragraphs = paragraphs.to(device)\n",
    "        paragraph_mask = paragraph_mask.to(device)\n",
    "        questions = questions.to(device)\n",
    "        question_mask = question_mask.to(device)\n",
    "        # span_list = span_list.to(device)\n",
    "\n",
    "        # forward pass, get the predictions\n",
    "        preds = model(paragraphs, questions, paragraph_mask, question_mask)\n",
    "\n",
    "        start_pred, end_pred = preds\n",
    "        # print('preds:', start_pred, end_pred)\n",
    "        start_pred_argmax = torch.argmax(start_pred, dim=1)\n",
    "        end_pred_argmax = torch.argmax(start_pred, dim=1)\n",
    "\n",
    "        # separate labels for start and end position\n",
    "        span_start = []\n",
    "        span_end = []\n",
    "        true_answers_list = []\n",
    "        my_answers = []\n",
    "        for paragraph, spans, answers, sp, ep in \\\n",
    "                zip(paragraphs, span_list, answer_list, start_pred_argmax, end_pred_argmax):\n",
    "            span_start.append([span[0].item() for span in spans][:3])\n",
    "            span_end.append([span[1].item() for span in spans][:3])\n",
    "            true_answers_list.append([ans2txt(answer) for answer in answers])\n",
    "            my_answers.append(span2txt([sp, ep + 1], paragraph))\n",
    "        with torch.no_grad():\n",
    "            # print('span:', span_start, span_end)\n",
    "            try:\n",
    "                span_start = torch.LongTensor(span_start).to(device)\n",
    "                span_end = torch.LongTensor(span_end).to(device)\n",
    "                # calculate loss\n",
    "                loss = F.cross_entropy(start_pred, span_start.t()[0]) + F.cross_entropy(end_pred, span_end.t()[0])\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                emScore += em_batch(my_answers, true_answers_list)\n",
    "                f1Score += f1_batch(my_answers, true_answers_list)\n",
    "            except:\n",
    "                print('start pred:', start_pred)\n",
    "                print('start pred shape:', start_pred.shape)\n",
    "                print('span_list:', span_list)\n",
    "                print('span_list length:', len(span_list))\n",
    "                print('span_start:', span_start)\n",
    "                print('span_start shape:', np.asarray(span_start).shape)\n",
    "                print('span_end:', span_end)\n",
    "                print(traceback.format_exc())\n",
    "\n",
    "    return val_loss / len(test_dataset), emScore / len(test_dataset), f1Score / len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "def normalize_answer(s):\n",
    "    '''\n",
    "    Performs a series of cleaning steps on the ground truth and\n",
    "    predicted answer.\n",
    "    '''\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "def em_batch(my_answers, true_answers_list):\n",
    "    # true_answers_list: batch size * 3\n",
    "    em = 0\n",
    "    for my_answer, true_answers in zip(my_answers, true_answers_list):\n",
    "        for true_answer in true_answers:\n",
    "            if my_answer == true_answer:\n",
    "                em += 1\n",
    "                break\n",
    "    return em / BATCH_SIZE\n",
    "\n",
    "def f1_batch(my_answers, true_answers_list):\n",
    "    f1Batch = 0\n",
    "    for my_answer, true_answers in zip(my_answers, true_answers_list):\n",
    "        f1_single = 0\n",
    "        for true_answer in true_answers:\n",
    "            my_answer_split = my_answer.split()\n",
    "            true_answer_split = true_answer.split()\n",
    "            common = Counter(my_answer_split) & Counter(true_answer_split)\n",
    "            num_same = sum(common.values())\n",
    "            if num_same == 0:\n",
    "                continue\n",
    "            precision = num_same / len(my_answer_split)\n",
    "            recall = num_same / len(true_answer_split)\n",
    "            f1_single += (2 * precision * recall) / (precision + recall)\n",
    "        f1Batch += f1_single / len(true_answers)\n",
    "    return f1Batch / BATCH_SIZE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def span2txt(span, paragraph):\n",
    "    # print(span[0])\n",
    "    my_answer = paragraph[span[0].item() : span[1].item() + 1]\n",
    "    return ans2txt(my_answer)\n",
    "def ans2txt(answer):\n",
    "    words = []\n",
    "    for a_index in answer:\n",
    "        words.append(vocab.itos[a_index.item()])\n",
    "    return normalize_answer(' '.join(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "best_val_loss = 100\n",
    "path = 'best.pt'\n",
    "if os.path.isfile(path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring epoch 0\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.012016057968139648\n",
      "Starting batch: 500, time: 120.70974564552307\n",
      "Starting batch: 1000, time: 248.5435757637024\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.003038644790649414\n",
      "train_loss: 7.550359774306472, val_loss: 6.927105261729314\n",
      "em_score: 11.165865384615385, f1_score: 19.701268404408893\n",
      "End epoch 0, elapsed time: 348.8913016319275\n",
      "Staring epoch 1\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.008999824523925781\n",
      "Starting batch: 500, time: 130.51600766181946\n",
      "Starting batch: 1000, time: 261.7660894393921\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.0029997825622558594\n",
      "train_loss: 6.511556327828295, val_loss: 6.080529135924119\n",
      "em_score: 15.264423076923078, f1_score: 27.48123479163016\n",
      "End epoch 1, elapsed time: 362.82775139808655\n",
      "Staring epoch 2\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.011000394821166992\n",
      "Starting batch: 500, time: 134.9014768600464\n",
      "Starting batch: 1000, time: 265.411199092865\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.00299072265625\n",
      "train_loss: 5.76977122580008, val_loss: 5.489334355867826\n",
      "em_score: 18.77403846153846, f1_score: 33.04713821961583\n",
      "End epoch 2, elapsed time: 372.89018988609314\n",
      "Staring epoch 3\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.006999015808105469\n",
      "Starting batch: 500, time: 134.44053888320923\n",
      "Starting batch: 1000, time: 266.2017734050751\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.002998828887939453\n",
      "train_loss: 5.2666665639460835, val_loss: 5.143810099821824\n",
      "em_score: 20.94951923076923, f1_score: 35.68023627932709\n",
      "End epoch 3, elapsed time: 368.175829410553\n",
      "Staring epoch 4\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.006999969482421875\n",
      "Starting batch: 500, time: 128.83278489112854\n",
      "Starting batch: 1000, time: 259.68961906433105\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.004004240036010742\n",
      "train_loss: 4.936835715013641, val_loss: 4.879585260611314\n",
      "em_score: 22.127403846153847, f1_score: 38.00453584828913\n",
      "End epoch 4, elapsed time: 361.57180285453796\n",
      "Staring epoch 5\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.007965087890625\n",
      "Starting batch: 500, time: 130.9073224067688\n",
      "Starting batch: 1000, time: 261.5354161262512\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.0030024051666259766\n",
      "train_loss: 4.706689447054238, val_loss: 4.729569763403672\n",
      "em_score: 22.716346153846153, f1_score: 39.08060032511786\n",
      "End epoch 5, elapsed time: 364.0656487941742\n",
      "Staring epoch 6\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.008999347686767578\n",
      "Starting batch: 500, time: 129.89021158218384\n",
      "Starting batch: 1000, time: 260.49133014678955\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.0020334720611572266\n",
      "train_loss: 4.532527055853301, val_loss: 4.6160926507069515\n",
      "em_score: 22.98076923076923, f1_score: 39.539912059182825\n",
      "End epoch 6, elapsed time: 361.03328251838684\n",
      "Staring epoch 7\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.01200103759765625\n",
      "Starting batch: 500, time: 127.92043495178223\n",
      "Starting batch: 1000, time: 257.42127299308777\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.003035306930541992\n",
      "train_loss: 4.379495949148691, val_loss: 4.521878046255845\n",
      "em_score: 24.290865384615383, f1_score: 41.49879358846871\n",
      "End epoch 7, elapsed time: 358.68527722358704\n",
      "Staring epoch 8\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.007999420166015625\n",
      "Starting batch: 500, time: 130.2681336402893\n",
      "Starting batch: 1000, time: 260.21188831329346\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.003001689910888672\n",
      "train_loss: 4.252143286670428, val_loss: 4.530373428418086\n",
      "em_score: 24.026442307692307, f1_score: 41.14889315497216\n",
      "End epoch 8, elapsed time: 360.99449396133423\n",
      "Staring epoch 9\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.01200246810913086\n",
      "Starting batch: 500, time: 129.64511156082153\n",
      "Starting batch: 1000, time: 258.36238718032837\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.0030028820037841797\n",
      "train_loss: 4.137033875477393, val_loss: 4.434475438411419\n",
      "em_score: 23.653846153846153, f1_score: 41.105741951017684\n",
      "End epoch 9, elapsed time: 360.26381158828735\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "em_scores = []\n",
    "f1_scores = []\n",
    "for epoch in range(10):\n",
    "    print(f'Staring epoch {epoch}')\n",
    "    start_time = time()\n",
    "\n",
    "    train_loss = train(model, trainloader)\n",
    "    val_loss, emScore, f1Score = validate(model, testloader)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    em_scores.append(em_scores)\n",
    "    f1_scores.append(f1_scores)\n",
    "\n",
    "    if best_val_loss > val_loss:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "            }, path)\n",
    "    end_time = time()\n",
    "\n",
    "    time_elapsed = end_time - start_time\n",
    "    print(f'train_loss: {train_loss}, val_loss: {val_loss}')\n",
    "    print(f'em_score: {emScore * 100}, f1_score: {f1Score * 100}')\n",
    "    print(f'End epoch {epoch}, elapsed time: {time_elapsed}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# val_loss, emScore, f1Score = validate(model, testloader)\n",
    "# print(f'train_loss: {train_loss}, val_loss: {val_loss}')\n",
    "# print(f'em_score: {emScore * 100}, f1_score: {f1Score * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot([i for i in range(1, 11)], train_losses)\n",
    "plt.plot([i for i in range(1, 11)], val_losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('CE losses')\n",
    "plt.title('Training Result')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# from matplotlib import pyplot as plt\n",
    "plt.plot([i for i in range(1, 11)], em_scores)\n",
    "plt.plot([i for i in range(1, 11)], f1_scores)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Scores on SQuAD 1.1')\n",
    "plt.legend(['EM', 'F1'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# idx = 0\n",
    "# for i, t_data in enumerate(testloader):\n",
    "#     idx = i\n",
    "# print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# (p, q, s, a, p_mask, q_mask), p 길이 점점 증가\n",
    "# p, q: tensor, batch size * length\n",
    "# s: tuple of tensors, batch size\n",
    "# a:\n",
    "dataiter = iter(trainloader)\n",
    "dataiter_next = dataiter.next()\n",
    "dataiter_next[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "for i in range(100):\n",
    "    dataiter_100 = dataiter.next()\n",
    "dataiter_100[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "paragraphs, questions, span_list, answer_list, paragraph_mask, question_mask = dataiter_next\n",
    "model_ex = DocumentReader(HIDDEN_SIZE,\n",
    "                       EMB_SIZE,\n",
    "                       NLAYERS,\n",
    "                       DROPOUT,\n",
    "                       device).to(device)\n",
    "model_ex.train()\n",
    "paragraphs = paragraphs.to(device)\n",
    "paragraph_mask = paragraph_mask.to(device)\n",
    "questions = questions.to(device)\n",
    "question_mask = question_mask.to(device)\n",
    "# span_list = span_list.to(device)\n",
    "\n",
    "# forward pass, get the predictions\n",
    "preds = model(paragraphs, questions, paragraph_mask, question_mask)\n",
    "\n",
    "start_pred, end_pred = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# start_pred.shape\n",
    "# start_pred_argmax = torch.argmax(start_pred, dim=1)\n",
    "# print(start_pred_argmax)\n",
    "# print(span_list)\n",
    "# print(span_list[0][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for p, s, a in zip(paragraphs, span_list, answer_list):\n",
    "    i += 1\n",
    "    if i < 3:\n",
    "        print(p)\n",
    "        print(s)\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# em_score = 0\n",
    "# for i, (p, s, a) in enumerate(list(zip(paragraphs, span_list, answer_list))):\n",
    "#     i += 1\n",
    "#     # print(i)\n",
    "#     if i < 3:\n",
    "#         # print(p)\n",
    "#         # print(s)\n",
    "#         # print(a)\n",
    "#         em_score += em_func(p, a[0], s)\n",
    "# print(em_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
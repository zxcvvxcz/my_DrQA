{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# trainloader length: 2700\n",
    "# testloader length: 323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "import os\n",
    "import traceback\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [00:22<00:00, 3900.56lines/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.experimental.datasets import SQuAD1\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "# data_dir = '.data'\n",
    "# data_names = ['dev-v1.1.json', 'train-v1.1.json']\n",
    "# for data_name in data_names:\n",
    "#     if not os.path.isfile(os.path.join(data_dir, data_name)):\n",
    "#         print('download')\n",
    "#         train, dev = SQuAD1()\n",
    "#         break\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "# dataset shape: (paragraph, question, answer, span)\n",
    "trainset, devset = SQuAD1(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab = trainset.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# errors = 0\n",
    "# print('length of vocab before filtering:', len(vocab.stoi))\n",
    "# for key, value in list(vocab.stoi.items()):\n",
    "#     if re.search('\\n', key) or re.search(' ', key):\n",
    "#         errors += 1\n",
    "#         print(key)\n",
    "#         vocab.stoi.pop(key)\n",
    "#         vocab.itos.pop(value)\n",
    "#         vocab.freqs.pop(key)\n",
    "#         # vocab.freqs[key] -= 1\n",
    "#         # if vocab.freqs[key] < 1:\n",
    "#         #     vocab.freqs.pop(key)\n",
    "#\n",
    "# print(errors)\n",
    "# print('length of vocab after filtering:', len(vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# trainset, devset = SQuAD1(vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_large_text(data):\n",
    "    return data[0] <= 400\n",
    "\n",
    "def check_train_data(data):\n",
    "    # data might be wrong because of spacy tokenizer\n",
    "    p_length, q_length, idx, paragraph, question, answer, span = data\n",
    "    if span[0][0] > p_length or span[0][1] > p_length:\n",
    "        return False\n",
    "    if paragraph[span[0][0]] == answer[0][0] and paragraph[span[0][1]] == answer[0][-1]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_dev_data(data):\n",
    "    p_length, q_length, idx, paragraph, question, answers, spans = data\n",
    "    if len(spans) != 3 or len(answers) != 3:\n",
    "        return False\n",
    "    else:\n",
    "        for span, answer in zip(spans, answers):\n",
    "            if span[0] > p_length or span[1] > p_length:\n",
    "                return False\n",
    "            if paragraph[span[0]] != answer[0] or paragraph[span[1]] != answer[-1]:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86412\n",
      "8295\n"
     ]
    }
   ],
   "source": [
    "train_data = [(len(paragraph), len(question), idx, paragraph, question, answer, span)\n",
    "            for idx, (paragraph, question, answer, span) in enumerate(trainset)]\n",
    "dev_data = [(len(paragraph), len(question), idx, paragraph, question, answer, span)\n",
    "            for idx, (paragraph, question, answer, span) in enumerate(devset)]\n",
    "\n",
    "train_data = list(filter(remove_large_text, train_data))\n",
    "dev_data = list(filter(remove_large_text, dev_data))\n",
    "\n",
    "train_data = list(filter(check_train_data, train_data))\n",
    "dev_data = list(filter(check_dev_data, dev_data))\n",
    "\n",
    "\n",
    "train_data.sort() # sort by length and pad sequences with similar lengths\n",
    "dev_data.sort()\n",
    "# paragraph, question: tensor of indices of words, use itos to get word\n",
    "print(len(train_data))\n",
    "print(len(dev_data))\n",
    "# Generate the pad id\n",
    "pad_id = vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(train_data[0][3])\n",
    "# for idx in train_data[0][3]:\n",
    "#     print(train.get_vocab().itos[idx], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pad_data(data):\n",
    "    # Find max length of the mini-batch\n",
    "    # train.get_vocab()['pad'], dev.get_vocab()['pad'] is equal to 22949\n",
    "    max_p_len = max(list(zip(*data))[0])\n",
    "    max_q_len = max(list(zip(*data))[1])\n",
    "    paragraph_list = list(zip(*data))[3]\n",
    "    question_list = list(zip(*data))[4]\n",
    "    answer_list = list(zip(*data))[5]\n",
    "    span_list = list(zip(*data))[6]\n",
    "    padded_paragraphs = torch.stack([torch.cat((paragraph,\n",
    "            torch.LongTensor([pad_id] * (max_p_len - len(paragraph))))) \\\n",
    "            for paragraph in paragraph_list])\n",
    "    padded_questions = torch.stack([torch.cat((question,\n",
    "            torch.tensor([pad_id] * (max_q_len - len(question))).long())) \\\n",
    "            for question in question_list])\n",
    "    paragraph_pad_mask = torch.zeros_like(padded_paragraphs).masked_fill(padded_paragraphs == pad_id, 1)\n",
    "    question_pad_mask = torch.zeros_like(padded_questions).masked_fill(padded_questions == pad_id, 1)\n",
    "\n",
    "    return padded_paragraphs, padded_questions, span_list, answer_list, \\\n",
    "           paragraph_pad_mask, question_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "from torch.utils.data import DataLoader\n",
    "trainloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_data)\n",
    "testloader = DataLoader(dev_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([12163, 56789]), tensor([12163, 56789]), tensor([12163, 56789])] [tensor([33, 34]), tensor([33, 34]), tensor([33, 34])]\n",
      "facing\n",
      "it\n",
      "[tensor([  881, 20129]), tensor([  881, 20129]), tensor([  881, 20129])] [tensor([44, 45]), tensor([44, 45]), tensor([44, 45])]\n",
      "upraised\n",
      "with\n"
     ]
    }
   ],
   "source": [
    "for i, (p, q, a, s) in enumerate(devset):\n",
    "    print(a,s)\n",
    "    nps = s[0].numpy()\n",
    "    tokens = tokenizer(trainset.data[i][0])\n",
    "    print(tokens[int(nps[0])])\n",
    "    print(tokens[nps[1]])\n",
    "    # print(tokens[a[0].numpy().item()])\n",
    "    if i > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for idx, (padded_paragraphs, padded_questions, span_list, answer_list,\n",
    "           paragraph_pad_mask, question_pad_mask) in enumerate(trainloader):\n",
    "    # print(idx, padded_paragraphs, padded_questions, span_list, answer_list,\n",
    "    #        paragraph_pad_mask, question_pad_mask)\n",
    "    # print(padded_paragraphs.masked_fill(paragraph_pad_mask == 1, -1))\n",
    "    if idx > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(trainset.get_vocab()['pad'], dev.get_vocab()['pad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "glove_vec = torchtext.vocab.GloVe(name='840B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_word_embedding(vocab, pre_trained_emb_vec):\n",
    "    # print(pre_trained_emb_vec.dim)\n",
    "    weights_matrix = np.zeros((len(vocab), pre_trained_emb_vec.dim))\n",
    "    words_found = 0\n",
    "    no_word = 0\n",
    "    for i, (word, _) in enumerate(vocab.freqs.most_common()):\n",
    "        try:\n",
    "            word_index = pre_trained_emb_vec.stoi[word]\n",
    "            weights_matrix[i] = pre_trained_emb_vec[word_index]\n",
    "            words_found += 1\n",
    "        except:\n",
    "            no_word += 1 # no such word in pre_trained_embedding: zero vector\n",
    "    print('words not found:', no_word)\n",
    "    print('words found:', words_found)\n",
    "    return torch.FloatTensor(weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for key, value in vocab.freqs.items():\n",
    "#     if re.search(' ', key):\n",
    "#         print(key, value)\n",
    "# for i, word in enumerate(vocab.freqs.most_common()):\n",
    "#     print(word)\n",
    "#     if i > 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words not found: 17435\n",
      "words found: 86591\n"
     ]
    }
   ],
   "source": [
    "word_emb_table = build_word_embedding(vocab, glove_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# glove_vec.vectors[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# not using now\n",
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['parser','ner',])\n",
    "#\n",
    "# def exact_match(paragraphs_indices, questions_indices, vocab):\n",
    "#     # process one paragraph batch, one question batch\n",
    "#     # print(paragraphs_indices.size())\n",
    "#     # print(questions_indices.size())\n",
    "#     #\n",
    "#     # j = 0\n",
    "#     # for (paragraph_indices, question_indices) in \\\n",
    "#     #         zip(paragraphs_indices, questions_indices):\n",
    "#     #     j += 1\n",
    "#     # print('j:',j)\n",
    "#     exact_match_table = np.zeros((len(paragraphs_indices), len(paragraphs_indices[0]), 3))\n",
    "#     # print(exact_match_table.shape)\n",
    "#\n",
    "#     for i, (paragraph_indices, question_indices) in \\\n",
    "#             enumerate(zip(paragraphs_indices, questions_indices)):\n",
    "#         # print(paragraphs_indices)\n",
    "#         # print(paragraphs_indices.size())\n",
    "#         # paragraph_processed = nlp(paragraph_sentence)\n",
    "#         # question_lemmas = [lem.lemma_ for lem in question_processed]\n",
    "#         for j, paragraph_index in enumerate(paragraph_indices):\n",
    "#             paragraph_word = vocab.itos[paragraph_index]\n",
    "#             if paragraph_word == '<pad>':\n",
    "#                 # print('got pad')\n",
    "#                 continue\n",
    "#             em_tensor = torch.LongTensor([0, 0, 0])\n",
    "#             # original\n",
    "#             if paragraph_index in question_indices:\n",
    "#                 em_tensor[0] = 1\n",
    "#             # lemma\n",
    "#             if vocab.stoi[nlp(paragraph_word)[0].lemma_] in question_indices:\n",
    "#                 em_tensor[1] = 1\n",
    "#             # uncased\n",
    "#             if vocab.stoi[paragraph_word.lower()] and \\\n",
    "#                     vocab.stoi[paragraph_word.lower()] in question_indices:\n",
    "#                 em_tensor[2] = 1\n",
    "#             exact_match_table[i][j] = em_tensor\n",
    "#\n",
    "#     return torch.LongTensor(exact_match_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AlignedQuestionEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, paragraph, question, question_pad_mask):\n",
    "\n",
    "        p = self.relu(self.linear(paragraph))\n",
    "\n",
    "        q = self.relu(self.linear(question))\n",
    "        q = q.permute(0, 2, 1)\n",
    "\n",
    "        dot_product = torch.bmm(p, q)\n",
    "        # print(dot_product.size())\n",
    "        # print(question_pad_mask.size())\n",
    "        question_mask_expand = question_pad_mask.unsqueeze(1).expand(dot_product.size())\n",
    "        dot_product = dot_product.masked_fill(question_mask_expand == 1, -float('inf'))\n",
    "\n",
    "        dot_product_flatten = dot_product.view(-1, question.size(1))\n",
    "\n",
    "        attn_score = F.softmax(dot_product_flatten, dim=1)\n",
    "        attn_score = attn_score.view(-1, paragraph.shape[1], question.shape[1])\n",
    "\n",
    "        aligned_embedding = torch.bmm(attn_score, question)\n",
    "        return aligned_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiLayerBiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nlayers, dropout):\n",
    "        super().__init__()\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "        self.lstms = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstms.append(nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True))\n",
    "        for i in range(1, nlayers):\n",
    "            self.lstms.append(nn.LSTM(hidden_size * 2, hidden_size,\n",
    "                                      batch_first=True, bidirectional=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.dropout(x)\n",
    "        lstm_output, (_, _) = self.lstms[0](x)\n",
    "        hidden_states = [lstm_output]\n",
    "        # print(lstm_output.size(), hidden_state.size(), cell_state.size())\n",
    "        for i in range(1, self.nlayers):\n",
    "            # lstm_output = self.dropout(lstm_output)\n",
    "            lstm_output, (_, _) = self.lstms[i](lstm_output)\n",
    "            # print(lstm_output.size(), hidden_state.size(), cell_state.size())\n",
    "            hidden_states.append(lstm_output)\n",
    "\n",
    "        output = torch.cat(hidden_states, dim=2)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QuestionEncoding(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, nlayers, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm_output_size = hidden_size * 6\n",
    "        self.linear = nn.Linear(self.lstm_output_size, 1)\n",
    "        self.lstm = MultiLayerBiLSTM(input_size, hidden_size, nlayers, dropout)\n",
    "        # biLSTM output size: hidden size * 6\n",
    "    def forward(self, x, question_mask):\n",
    "        try:\n",
    "            x = self.lstm(x)\n",
    "            b = x.view(-1, self.lstm_output_size)\n",
    "            b = self.linear(b) # attention score\n",
    "            b = b.view(question_mask.shape[0], -1)\n",
    "            # print(x.size(), question_mask.size())\n",
    "            b = b.masked_fill(question_mask == 1, -float('inf')) # masking\n",
    "            b = F.softmax(b, dim=1)\n",
    "\n",
    "            b = b.unsqueeze(1)\n",
    "            # print(x.size(), x_lstm.size())\n",
    "            encoding = torch.bmm(b, x)\n",
    "            encoding = encoding.squeeze(1)\n",
    "            return encoding\n",
    "        except:\n",
    "            print('question mask size:', question_mask.size())\n",
    "            print('x size:', x.size())\n",
    "            print('b size:', b.size())\n",
    "            print(traceback.print_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PredictionLayer(nn.Module):\n",
    "    def __init__(self, p_size, q_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(q_size, p_size)\n",
    "\n",
    "    def forward(self, paragraph, question, paragraph_mask):\n",
    "        Wq = self.linear(question)\n",
    "        Wq = Wq.unsqueeze(2)\n",
    "        pWq = paragraph.bmm(Wq)\n",
    "        pWq = pWq.squeeze(2)\n",
    "        pWq = pWq.masked_fill(paragraph_mask == 1, -float('inf'))\n",
    "        return pWq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fixate_embedding(grad):\n",
    "    grad[1000:] = 0\n",
    "    return grad\n",
    "\n",
    "class DocumentReader(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, nlayers, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.word_embedding_layer = nn.Embedding.from_pretrained(torch.FloatTensor(word_emb_table).to(device), freeze=False)\n",
    "        self.word_embedding_layer.weight.register_hook(fixate_embedding)\n",
    "        # print(embedding_size)\n",
    "        self.aligned_embedding_layer = AlignedQuestionEmbedding(embedding_size)\n",
    "        # self.paragraph_lstm = MultiLayerBiLSTM(embedding_size * 2 + 3, hidden_size, nlayers, dropout)\n",
    "        self.paragraph_lstm = MultiLayerBiLSTM(embedding_size * 2, hidden_size, nlayers, dropout)\n",
    "\n",
    "        self.question_encoder = QuestionEncoding(embedding_size, hidden_size, nlayers, dropout)\n",
    "\n",
    "        self.prediction_layer_start = PredictionLayer(hidden_size * nlayers * 2,\n",
    "                                                          hidden_size * nlayers * 2)\n",
    "        self.prediction_layer_end = PredictionLayer(hidden_size * nlayers * 2,\n",
    "                                                        hidden_size * nlayers * 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, paragraph, question, paragraph_mask, question_mask):\n",
    "        # em_embedding = exact_match(paragraph, question, vocab)\n",
    "        # print(em_embedding.size())\n",
    "        p_word_embedding = self.word_embedding_layer(paragraph)\n",
    "        q_word_embedding = self.word_embedding_layer(question)\n",
    "        p_word_embedding = self.dropout(p_word_embedding)\n",
    "        q_word_embedding = self.dropout(q_word_embedding)\n",
    "        aligned_embedding = self.aligned_embedding_layer(p_word_embedding, q_word_embedding, question_mask)\n",
    "        # print(p_word_embedding.size())\n",
    "        # print(aligned_embedding.size())\n",
    "        paragraph_embeddings = torch.cat([p_word_embedding, aligned_embedding], dim=2)\n",
    "\n",
    "        # paragraph_embeddings = torch.cat([em_embedding.to(device), p_word_embedding.to(device), aligned_embedding.to(device)], dim=2)\n",
    "        paragraph_encoding = self.paragraph_lstm(paragraph_embeddings)\n",
    "        # print(question.size(), question_mask.size())\n",
    "        question_encoding = self.question_encoder(q_word_embedding, question_mask)\n",
    "\n",
    "        prediction_start = self.prediction_layer_start(paragraph_encoding, question_encoding, paragraph_mask)\n",
    "        prediction_end = self.prediction_layer_end(paragraph_encoding, question_encoding, paragraph_mask)\n",
    "\n",
    "        return prediction_start, prediction_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "EMB_SIZE = 300\n",
    "NLAYERS = 3\n",
    "DROPOUT = 0.3\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DocumentReader(HIDDEN_SIZE,\n",
    "                       EMB_SIZE,\n",
    "                       NLAYERS,\n",
    "                       DROPOUT,\n",
    "                       device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "#\n",
    "# writer = SummaryWriter('runs/myDrQA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataiter = iter(trainloader)\n",
    "# dataiter_next = dataiter.next()\n",
    "# print(dataiter_next)\n",
    "# (p, q, a, s, p_mask, q_mask) = dataiter.next()\n",
    "# writer.add_graph(model, p, p_mask, q_mask)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "def train(model, train_dataset):\n",
    "    '''\n",
    "    Trains the model.\n",
    "    '''\n",
    "\n",
    "    print(\"Start training ........\")\n",
    "\n",
    "    train_loss = 0.\n",
    "\n",
    "    # put the model in training mode\n",
    "    model.train()\n",
    "    start_time = time()\n",
    "    # iterate through training data\n",
    "    try:\n",
    "        for i, (paragraphs, questions, span_list, answer_list,\n",
    "                paragraph_mask, question_mask) in enumerate(train_dataset):\n",
    "            # if i < 575:\n",
    "            #     continue\n",
    "            if i % 500 == 0:\n",
    "                print(f\"Starting batch: {i}, time: {time() - start_time}\")\n",
    "\n",
    "\n",
    "            # place the tensors on GPU\n",
    "            paragraphs = paragraphs.to(device)\n",
    "            paragraph_mask = paragraph_mask.to(device)\n",
    "            questions = questions.to(device)\n",
    "            question_mask = question_mask.to(device)\n",
    "            # span_list = span_list.to(device)\n",
    "\n",
    "            # forward pass, get the predictions\n",
    "            preds = model(paragraphs, questions, paragraph_mask, question_mask)\n",
    "\n",
    "            start_pred, end_pred = preds\n",
    "\n",
    "            # print('preds:', start_pred, end_pred)\n",
    "            # separate labels for start and end position\n",
    "            span_start = []\n",
    "            span_end = []\n",
    "            for span in span_list:\n",
    "                span_start.append(span[0][0].item())\n",
    "                span_end.append(span[0][1].item())\n",
    "\n",
    "            # print('span:', span_start, span_end)\n",
    "            span_start = torch.LongTensor(span_start).to(device)\n",
    "            span_end = torch.LongTensor(span_end).to(device)\n",
    "            # calculate loss\n",
    "            loss = F.cross_entropy(start_pred, span_start) + F.cross_entropy(end_pred, span_end)\n",
    "\n",
    "            # backward pass, calculates the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "\n",
    "            # update the gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the gradients to prevent them from accumulating\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # ...학습 중 손실(running loss)을 기록하고\n",
    "            # writer.add_scalar('training loss',\n",
    "            #                 train_loss / 500,\n",
    "            #                 epoch * len(trainloader) + i)\n",
    "            train_loss += loss.item()\n",
    "    except Exception as e:\n",
    "        print(f'sizes of pred:{start_pred.size()} / span:{span_start.size()}')\n",
    "        print(f'span_start: {span_start[23]}\\nspan_end: {span_end[23]}')\n",
    "        print(f'i: {i}')\n",
    "        print(f'paragraph: {paragraphs}')\n",
    "        bad_p = paragraphs.numpy()[23]\n",
    "        bad_q = questions.numpy()[23]\n",
    "        bad_p_text = [vocab.itos[pi] for pi in bad_p]\n",
    "        bad_q_text = [vocab.itos[qi] for qi in bad_q]\n",
    "        bad_p_text = ' '.join(bad_p_text)\n",
    "        bad_q_text = ' '.join(bad_q_text)\n",
    "\n",
    "        print(bad_p_text)\n",
    "        print(bad_q_text)\n",
    "        print(f'paragraph size: {paragraphs.size()}, question size: {questions.size()}')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "\n",
    "    return train_loss / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %time train_loss = train(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def validate(model, test_dataset):\n",
    "    '''\n",
    "    Validates the model.\n",
    "    '''\n",
    "\n",
    "    print(\"Start validation ........\")\n",
    "\n",
    "    val_loss = 0.\n",
    "    emScore = 0\n",
    "    f1Score = 0\n",
    "    # put the model in eval mode\n",
    "    model.eval()\n",
    "    start_time = time()\n",
    "    # iterate through training data\n",
    "    for i, (paragraphs, questions, span_list, answer_list,\n",
    "            paragraph_mask, question_mask) in enumerate(test_dataset):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Starting batch: {i}, time: {time() - start_time}\")\n",
    "\n",
    "\n",
    "        # place the tensors on GPU\n",
    "        paragraphs = paragraphs.to(device)\n",
    "        paragraph_mask = paragraph_mask.to(device)\n",
    "        questions = questions.to(device)\n",
    "        question_mask = question_mask.to(device)\n",
    "        # span_list = span_list.to(device)\n",
    "\n",
    "        # forward pass, get the predictions\n",
    "        preds = model(paragraphs, questions, paragraph_mask, question_mask)\n",
    "\n",
    "        start_pred, end_pred = preds\n",
    "        # print('preds:', start_pred, end_pred)\n",
    "        start_pred_argmax = torch.argmax(start_pred, dim=1)\n",
    "        end_pred_argmax = torch.argmax(start_pred, dim=1)\n",
    "\n",
    "        # separate labels for start and end position\n",
    "        span_start = []\n",
    "        span_end = []\n",
    "        true_answers_list = []\n",
    "        my_answers = []\n",
    "        for paragraph, spans, answers, sp, ep in \\\n",
    "                zip(paragraphs, span_list, answer_list, start_pred_argmax, end_pred_argmax):\n",
    "            span_start.append([span[0].item() for span in spans][:3])\n",
    "            span_end.append([span[1].item() for span in spans][:3])\n",
    "            true_answers_list.append([ans2txt(answer) for answer in answers])\n",
    "            my_answers.append(span2txt([sp, ep + 1], paragraph))\n",
    "        with torch.no_grad():\n",
    "            # print('span:', span_start, span_end)\n",
    "            try:\n",
    "                span_start = torch.LongTensor(span_start).to(device)\n",
    "                span_end = torch.LongTensor(span_end).to(device)\n",
    "                # calculate loss\n",
    "                loss = F.cross_entropy(start_pred, span_start.t()[0]) + F.cross_entropy(end_pred, span_end.t()[0])\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                emScore += em_batch(my_answers, true_answers_list)\n",
    "                f1Score += f1_batch(my_answers, true_answers_list)\n",
    "            except:\n",
    "                print('start pred:', start_pred)\n",
    "                print('start pred shape:', start_pred.shape)\n",
    "                print('span_list:', span_list)\n",
    "                print('span_list length:', len(span_list))\n",
    "                print('span_start:', span_start)\n",
    "                print('span_start shape:', np.asarray(span_start).shape)\n",
    "                print('span_end:', span_end)\n",
    "                print(traceback.format_exc())\n",
    "\n",
    "    return val_loss / len(test_dataset), emScore / len(test_dataset), f1Score / len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "def normalize_answer(s):\n",
    "    '''\n",
    "    Performs a series of cleaning steps on the ground truth and\n",
    "    predicted answer.\n",
    "    '''\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "def em_batch(my_answers, true_answers_list):\n",
    "    # true_answers_list: batch size * 3\n",
    "    em = 0\n",
    "    for my_answer, true_answers in zip(my_answers, true_answers_list):\n",
    "        for true_answer in true_answers:\n",
    "            if my_answer == true_answer:\n",
    "                em += 1\n",
    "                break\n",
    "    return em / BATCH_SIZE\n",
    "\n",
    "def f1_batch(my_answers, true_answers_list):\n",
    "    f1Batch = 0\n",
    "    for my_answer, true_answers in zip(my_answers, true_answers_list):\n",
    "        f1_single = 0\n",
    "        for true_answer in true_answers:\n",
    "            my_answer_split = my_answer.split()\n",
    "            true_answer_split = true_answer.split()\n",
    "            common = Counter(my_answer_split) & Counter(true_answer_split)\n",
    "            num_same = sum(common.values())\n",
    "            if num_same == 0:\n",
    "                continue\n",
    "            precision = num_same / len(my_answer_split)\n",
    "            recall = num_same / len(true_answer_split)\n",
    "            f1_single += (2 * precision * recall) / (precision + recall)\n",
    "        f1Batch += f1_single / len(true_answers)\n",
    "    return f1Batch / BATCH_SIZE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def span2txt(span, paragraph):\n",
    "    # print(span[0])\n",
    "    my_answer = paragraph[span[0].item() : span[1].item() + 1]\n",
    "    return ans2txt(my_answer)\n",
    "def ans2txt(answer):\n",
    "    words = []\n",
    "    for a_index in answer:\n",
    "        words.append(vocab.itos[a_index.item()])\n",
    "    return normalize_answer(' '.join(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "best_val_loss = 100\n",
    "path = 'best.pt'\n",
    "if os.path.isfile(path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring epoch 0\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.007999420166015625\n",
      "Starting batch: 500, time: 111.88347554206848\n",
      "Starting batch: 1000, time: 232.55780577659607\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.0039997100830078125\n",
      "train_loss: 7.522219305299636, val_loss: 6.809384606434748\n",
      "em_score: 11.466346153846153, f1_score: 20.633015914072914\n",
      "End epoch 0, elapsed time: 329.16336488723755\n",
      "Staring epoch 1\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.008033990859985352\n",
      "Starting batch: 500, time: 126.66039276123047\n",
      "Starting batch: 1000, time: 255.39749121665955\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.003994464874267578\n",
      "train_loss: 6.385463044345158, val_loss: 5.948482212653527\n",
      "em_score: 15.805288461538462, f1_score: 28.596841912320713\n",
      "End epoch 1, elapsed time: 354.76946210861206\n",
      "Staring epoch 2\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.015001773834228516\n",
      "Starting batch: 500, time: 132.3486087322235\n",
      "Starting batch: 1000, time: 262.5170543193817\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.001949310302734375\n",
      "train_loss: 5.6310131516304835, val_loss: 5.358412621571468\n",
      "em_score: 19.09855769230769, f1_score: 33.311287814987864\n",
      "End epoch 2, elapsed time: 365.8027288913727\n",
      "Staring epoch 3\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.011000394821166992\n",
      "Starting batch: 500, time: 130.40596628189087\n",
      "Starting batch: 1000, time: 262.0919864177704\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.0039615631103515625\n",
      "train_loss: 5.116180356037696, val_loss: 5.044002032279968\n",
      "em_score: 21.40625, f1_score: 36.80586823859546\n",
      "End epoch 3, elapsed time: 366.6354613304138\n",
      "Staring epoch 4\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.008000612258911133\n",
      "Starting batch: 500, time: 133.04827046394348\n",
      "Starting batch: 1000, time: 267.7692914009094\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.002955913543701172\n",
      "train_loss: 4.803921525518953, val_loss: 4.832255099369929\n",
      "em_score: 22.235576923076923, f1_score: 38.74962903575166\n",
      "End epoch 4, elapsed time: 371.9961357116699\n",
      "Staring epoch 5\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.010001897811889648\n",
      "Starting batch: 500, time: 128.2801468372345\n",
      "Starting batch: 1000, time: 260.25391936302185\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.0029561519622802734\n",
      "train_loss: 4.585141536485346, val_loss: 4.691672989038321\n",
      "em_score: 23.052884615384617, f1_score: 39.485580381159366\n",
      "End epoch 5, elapsed time: 362.7239179611206\n",
      "Staring epoch 6\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.013971567153930664\n",
      "Starting batch: 500, time: 131.4209589958191\n",
      "Starting batch: 1000, time: 264.6416401863098\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.003021240234375\n",
      "train_loss: 4.406859863783852, val_loss: 4.603068146338829\n",
      "em_score: 23.485576923076923, f1_score: 40.22322843214404\n",
      "End epoch 6, elapsed time: 369.8233880996704\n",
      "Staring epoch 7\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.00799560546875\n",
      "Starting batch: 500, time: 141.88251972198486\n",
      "Starting batch: 1000, time: 301.07593297958374\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.0040395259857177734\n",
      "train_loss: 4.250903995190613, val_loss: 4.506664776802063\n",
      "em_score: 23.509615384615383, f1_score: 40.38048960494263\n",
      "End epoch 7, elapsed time: 425.25433921813965\n",
      "Staring epoch 8\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.014039278030395508\n",
      "Starting batch: 500, time: 141.86418652534485\n",
      "Starting batch: 1000, time: 281.6639199256897\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.0030024051666259766\n",
      "train_loss: 4.107916451841703, val_loss: 4.487594609994154\n",
      "em_score: 24.302884615384617, f1_score: 41.465582217515085\n",
      "End epoch 8, elapsed time: 389.03807640075684\n",
      "Staring epoch 9\n",
      "Start training ........\n",
      "Starting batch: 0, time: 0.013996362686157227\n",
      "Starting batch: 500, time: 137.81290936470032\n",
      "Starting batch: 1000, time: 276.0019705295563\n",
      "Start validation ........\n",
      "Starting batch: 0, time: 0.003000497817993164\n",
      "train_loss: 3.9728922782166634, val_loss: 4.43792844919058\n",
      "em_score: 25.02403846153846, f1_score: 42.15855054886441\n",
      "End epoch 9, elapsed time: 385.1567351818085\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "em_scores = []\n",
    "f1_scores = []\n",
    "for epoch in range(10):\n",
    "    print(f'Staring epoch {epoch}')\n",
    "    start_time = time()\n",
    "\n",
    "    train_loss = train(model, trainloader)\n",
    "    val_loss, emScore, f1Score = validate(model, testloader)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    em_scores.append(em_scores)\n",
    "    f1_scores.append(f1_scores)\n",
    "\n",
    "    if best_val_loss > val_loss:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "            }, path)\n",
    "    end_time = time()\n",
    "\n",
    "    time_elapsed = end_time - start_time\n",
    "    print(f'train_loss: {train_loss}, val_loss: {val_loss}')\n",
    "    print(f'em_score: {emScore * 100}, f1_score: {f1Score * 100}')\n",
    "    print(f'End epoch {epoch}, elapsed time: {time_elapsed}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# val_loss, emScore, f1Score = validate(model, testloader)\n",
    "# print(f'train_loss: {train_loss}, val_loss: {val_loss}')\n",
    "# print(f'em_score: {emScore * 100}, f1_score: {f1Score * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot([i for i in range(1, 11)], train_losses)\n",
    "plt.plot([i for i in range(1, 11)], val_losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('CE losses')\n",
    "plt.title('Training Result')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# from matplotlib import pyplot as plt\n",
    "plt.plot([i for i in range(1, 11)], em_scores)\n",
    "plt.plot([i for i in range(1, 11)], f1_scores)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Scores on SQuAD 1.1')\n",
    "plt.legend(['EM', 'F1'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# idx = 0\n",
    "# for i, t_data in enumerate(testloader):\n",
    "#     idx = i\n",
    "# print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# (p, q, s, a, p_mask, q_mask), p 길이 점점 증가\n",
    "# p, q: tensor, batch size * length\n",
    "# s: tuple of tensors, batch size\n",
    "# a:\n",
    "dataiter = iter(trainloader)\n",
    "dataiter_next = dataiter.next()\n",
    "dataiter_next[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "for i in range(100):\n",
    "    dataiter_100 = dataiter.next()\n",
    "dataiter_100[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "paragraphs, questions, span_list, answer_list, paragraph_mask, question_mask = dataiter_next\n",
    "model_ex = DocumentReader(HIDDEN_SIZE,\n",
    "                       EMB_SIZE,\n",
    "                       NLAYERS,\n",
    "                       DROPOUT,\n",
    "                       device).to(device)\n",
    "model_ex.train()\n",
    "paragraphs = paragraphs.to(device)\n",
    "paragraph_mask = paragraph_mask.to(device)\n",
    "questions = questions.to(device)\n",
    "question_mask = question_mask.to(device)\n",
    "# span_list = span_list.to(device)\n",
    "\n",
    "# forward pass, get the predictions\n",
    "preds = model(paragraphs, questions, paragraph_mask, question_mask)\n",
    "\n",
    "start_pred, end_pred = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# start_pred.shape\n",
    "# start_pred_argmax = torch.argmax(start_pred, dim=1)\n",
    "# print(start_pred_argmax)\n",
    "# print(span_list)\n",
    "# print(span_list[0][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for p, s, a in zip(paragraphs, span_list, answer_list):\n",
    "    i += 1\n",
    "    if i < 3:\n",
    "        print(p)\n",
    "        print(s)\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# em_score = 0\n",
    "# for i, (p, s, a) in enumerate(list(zip(paragraphs, span_list, answer_list))):\n",
    "#     i += 1\n",
    "#     # print(i)\n",
    "#     if i < 3:\n",
    "#         # print(p)\n",
    "#         # print(s)\n",
    "#         # print(a)\n",
    "#         em_score += em_func(p, a[0], s)\n",
    "# print(em_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}